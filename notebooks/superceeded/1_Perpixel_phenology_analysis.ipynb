{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b43d99-d698-40b4-a6cb-1e205a6cc0d7",
   "metadata": {},
   "source": [
    "# Per pixel phenology modelling for Australia\n",
    "\n",
    "This is very compute heavy and we can only return summaries (long term average, and/or trends etc.) as every pixel has a different length of seasons across the 40+ year archive, but it works and gives robust phenometrics for Aus.  Australia is split into eight tiles and each tile is run sequentially.\n",
    "\n",
    "In this notebook, all aspects of the analysis are run (phenology, trends, partial least squares regressions), but which analysis is run is configurable.\n",
    "\n",
    "Use a large local dask cluster, recommend `normalsr` queue and `104 cpus 496 GiB`, will take about 10 hours to loop through the 8 tiles.\n",
    "\n",
    "Some references for optimising processing:\n",
    "* https://github.com/NCI900-Training-Organisation/Distributed-Dask-Cluster-on-Gadi\n",
    "* https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a23e7-5a01-4839-a9c2-f02880a81e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import dask\n",
    "import scipy\n",
    "import warnings\n",
    "import dask.array\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from dask import delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from odc.geo.xr import assign_crs\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/Aus_phenology/src')\n",
    "from phenology_pixel import _preprocess, xr_phenometrics, phenology_trends, _mean, regression_attribution, IOS_analysis\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import round_coords\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304396c-c266-48cd-a707-e1dbca45d307",
   "metadata": {},
   "source": [
    "## Dask cluster\n",
    "\n",
    "Local or Dynamic?\n",
    "\n",
    "Dyanamic can be fickle so stick with local for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881177a6-0576-4ea2-a920-08193a096dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import start_local_dask\n",
    "start_local_dask(n_workers=100, threads_per_worker=1, memory_limit='400GiB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6a5f6-5bea-48af-8b7f-1ceaab3b7904",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# from dask_jobqueue import PBSCluster\n",
    "\n",
    "# cpus=13\n",
    "# mem='60GB'\n",
    "# extra = ['-q normalsr',\n",
    "#          '-P w97', \n",
    "#          '-l ncpus='+str(cpus), \n",
    "#          '-l mem='+mem,\n",
    "#         '-l storage=gdata/os22+gdata/w97'\n",
    "#         ]\n",
    "# setup_commands = [\"module load python3/3.10.0\", \"source /g/data/os22/chad_tmp/AusENDVI/env/py310/bin/activate\"]\n",
    "\n",
    "# cluster = PBSCluster(walltime=\"01:00:00\", \n",
    "#                      cores=cpus,\n",
    "#                      processes=cpus,\n",
    "#                      memory=mem,\n",
    "#                      shebang='#!/usr/bin/env bash',\n",
    "#                      job_extra_directives=extra, \n",
    "#                      local_directory='/g/data/os22/chad_tmp/Aus_phenology/data', \n",
    "#                      job_directives_skip=[\"select\"], \n",
    "#                      interface=\"ib0\",\n",
    "#                      job_script_prologue=setup_commands,\n",
    "#                     )\n",
    "\n",
    "# print(cluster.job_script())\n",
    "# cluster.scale(jobs=2)\n",
    "# client = Client(cluster)\n",
    "# client\n",
    "\n",
    "# client.shutdown()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e68da2-e310-41c5-82fd-c8f3ce96b310",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "Which aspects of the analysis should be run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58db45-62b6-42c7-9f14-b1390025958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "average = True\n",
    "trends = True\n",
    "ios = True\n",
    "regression = True\n",
    "regress_var = 'vPOS'\n",
    "model_type='delta_slope'\n",
    "modelling_vars=['co2', 'srad', 'rain', 'tavg', 'vpd']\n",
    "\n",
    "results_path = '/g/data/os22/chad_tmp/Aus_phenology/results/combined_tiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b77dc-62cc-445f-b650-72d104c6d175",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7391fc-5c9d-41fb-ad27-342d2badb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/g/data/os22/chad_tmp/Aus_phenology/data/NDVI/NDVI_smooth_AusENDVI-clim_MCD43A4.nc')['NDVI']\n",
    "covariables =  xr.open_dataset('/g/data/os22/chad_tmp/Aus_phenology/data/covars.nc')\n",
    "covariables = covariables.drop_vars('wcf')\n",
    "\n",
    "#testing slices\n",
    "ds = ds.isel(latitude=slice(200,352), longitude=slice(50,302)) \n",
    "covariables = covariables.isel(latitude=slice(200,352), longitude=slice(50,302))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5431ff9-08fd-4f58-a717-52f15d242139",
   "metadata": {},
   "source": [
    "## Split data into tiles\n",
    "\n",
    "Running all of Aus just takes too long, >500,000 pixels * > 14,000 time steps - dask graph is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e72dd-43e9-4fdc-9111-434c4a11a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split into spatial tiles\n",
    "def split_spatial_tiles(data_array, lat_dim='latitude', lon_dim='longitude', n_lat=2, n_lon=4):\n",
    "    lat_size = data_array.sizes[lat_dim] // n_lat\n",
    "    lon_size = data_array.sizes[lon_dim] // n_lon\n",
    "    \n",
    "    tiles = []\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            tile = data_array.isel({\n",
    "                lat_dim: slice(i * lat_size, (i + 1) * lat_size),\n",
    "                lon_dim: slice(j * lon_size, (j + 1) * lon_size)\n",
    "            })\n",
    "            tiles.append(tile)\n",
    "    \n",
    "    return tiles\n",
    "\n",
    "# Split data into spatial tiles (2 latitude x 4 longitude)\n",
    "tiles = split_spatial_tiles(ds, n_lat=2, n_lon=4)\n",
    "covars_tiles = split_spatial_tiles(covariables, n_lat=2, n_lon=4)\n",
    "\n",
    "#verify no overlaps or missing pixels.\n",
    "assert np.sum(xr.combine_by_coords(tiles).longitude == ds.longitude) == len(ds.longitude)\n",
    "assert np.sum(xr.combine_by_coords(tiles).latitude == ds.latitude) == len(ds.latitude)\n",
    "\n",
    "# create named dictonary\n",
    "tile_names=['NW', 'NNW', 'NNE', 'NE',\n",
    "            'SW', 'SSW', 'SSE', 'SE']\n",
    "tiles_dict = dict(zip(tile_names, tiles))\n",
    "covars_tiles_dict = dict(zip(tile_names, covars_tiles))\n",
    "\n",
    "#create a plot to visualise tiles\n",
    "# fig,axes = plt.subplots(2, 4, figsize=(10,8))\n",
    "# for t,ax in zip(tiles, axes.ravel()):\n",
    "#     t.isel(time=range(0,20)).mean('time').plot(ax=ax, add_colorbar=False, add_labels=False)\n",
    "#     ax.set_title(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85896-5100-41ac-a7e3-43d11e948099",
   "metadata": {},
   "source": [
    "## Per pixel phenometrics with dask.delayed\n",
    "\n",
    "Loop through the eight tiles and compute the time series of phenometerics, the average phenometrics, the trends in phenometrics, and PLS regression modelling.\n",
    "\n",
    "The tiles can be combined thereafter to have our continental per pixel phenology analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f7b48-42c7-4799-bbd6-a614475177ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (n,d),(nn,dd) in zip(tiles_dict.items(), covars_tiles_dict.items()):\n",
    "\n",
    "    #first lets check if the analysis has already been done\n",
    "    if os.path.exists(f'{results_path}attribution_{regress_var}_{model_type}_perpixel_{n}.nc'):\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        print('Working on tile: '+ n)\n",
    "    \n",
    "        # transform the data and return all the objects we need. This code smooth and\n",
    "        # interpolates the data, then stacks the pixels into a spatial index\n",
    "        d, dd, Y, idx_all_nan, nan_mask, shape = _preprocess(d, dd)\n",
    "        \n",
    "        # Open templates array which we'll use whenever we encounter an all-NaN index\n",
    "        # This speeds up the analysis by not running pixels that are empty.\n",
    "        # Created the template using one of the output results\n",
    "        # bb = xr.full_like(results[0], fill_value=np.nan, dtype='float32')\n",
    "        template_path='/g/data/os22/chad_tmp/Aus_phenology/data/templates/'\n",
    "        phen_template = xr.open_dataset(f'{template_path}template.nc')\n",
    "        ios_template = xr.open_dataset(f'{template_path}template_IOS.nc')\n",
    "        regress_template = xr.open_dataset(f'{template_path}template_{model_type}.nc').sel(feature=modelling_vars)\n",
    "    \n",
    "        #now we start the real proceessing\n",
    "        results=[]\n",
    "        for i in range(shape[1]): #loop through all spatial indexes.\n",
    "        \n",
    "            #select pixel\n",
    "            data = Y.isel(spatial=i)\n",
    "            \n",
    "            # First, check if spatial index has data. If its one of \n",
    "            # the all-NaN indexes then return xarray filled with -99 values\n",
    "            if i in idx_all_nan:\n",
    "                xx = phen_template.copy() #use our template    \n",
    "                xx['latitude'] = [data.latitude.values.item()] #update coords\n",
    "                xx['longitude'] = [data.longitude.values.item()]\n",
    "            \n",
    "            else:\n",
    "                #run the phenometrics\n",
    "                xx = xr_phenometrics(data,\n",
    "                                  rolling=90,\n",
    "                                  distance=90,\n",
    "                                  prominence='auto',\n",
    "                                  plateau_size=10,\n",
    "                                  amplitude=0.20\n",
    "                                 )\n",
    "        \n",
    "            #append results, either data or all-zeros\n",
    "            results.append(xx)\n",
    "        \n",
    "        # bring into memory.\n",
    "        results = dask.compute(results)[0]\n",
    "        \n",
    "        ### ----Summarise phenology with a median------------\n",
    "        ## This is one of the slowest steps.\n",
    "        if average:\n",
    "            if os.path.exists(f'{results_path}mean_phenology_perpixel_{n}.nc'):\n",
    "                pass\n",
    "            else:\n",
    "                p_average = [_mean(x) for x in results]\n",
    "                p_average = dask.compute(p_average)[0]\n",
    "                p_average = xr.combine_by_coords(p_average)\n",
    "                \n",
    "                #remove NaN areas that have a fill value\n",
    "                p_average = p_average.where(p_average>-99).astype('float32')\n",
    "                p_average = p_average.where(~np.isnan(p_average.vPOS)) #and again for the n_seasons layer\n",
    "                p_average = assign_crs(p_average, crs='EPSG:4326') # add geobox\n",
    "            \n",
    "                #export results\n",
    "                p_average.to_netcdf(f'{results_path}mean_phenology_perpixel_{n}.nc')\n",
    "    \n",
    "        ## ----Find the trends in phenology--------------\n",
    "        #now find trends in phenometrics\n",
    "        if trends:\n",
    "            if os.path.exists(f'{results_path}trends_phenology_perpixel_{n}.nc'):\n",
    "                pass\n",
    "            else:\n",
    "                trend_vars = ['POS','vPOS','TOS','vTOS','AOS','SOS','vSOS','EOS',\n",
    "                            'vEOS','LOS','IOS','ROG','ROS','LOS*vPOS','IOS:(LOS*vPOS)']\n",
    "                p_trends = [phenology_trends(x, trend_vars) for x in results]\n",
    "                p_trends = dask.compute(p_trends)[0]\n",
    "                p_trends = xr.combine_by_coords(p_trends)\n",
    "                \n",
    "                #remove NaNs\n",
    "                p_trends = p_trends.where(~np.isnan(p_average.vPOS)).astype('float32')\n",
    "            \n",
    "                # assign crs and export\n",
    "                p_trends = assign_crs(p_trends, crs='EPSG:4326')\n",
    "                p_trends.to_netcdf(f'{results_path}trends_phenology_perpixel_{n}.nc')\n",
    "    \n",
    "        # ----Partial correlation analysis etc. on IOS -----------------\n",
    "        if ios:\n",
    "            if os.path.exists(f'{results_path}IOS_analysis_perpixel_{n}.nc'):\n",
    "                pass\n",
    "            else:\n",
    "                p_parcorr = []\n",
    "                for pheno in results:            \n",
    "                    corr = IOS_analysis(pheno, ios_template)\n",
    "                    p_parcorr.append(corr)\n",
    "                \n",
    "                p_parcorr = dask.compute(p_parcorr)[0]\n",
    "                p_parcorr = xr.combine_by_coords(p_parcorr).astype('float32')\n",
    "                p_parcorr.to_netcdf(f'{results_path}IOS_analysis_perpixel_{n}.nc')\n",
    "    \n",
    "        # -----PLS regression-------------------------------\n",
    "        if regression:\n",
    "            if os.path.exists(f'{results_path}attribution_{regress_var}_{model_type}_perpixel_{n}.nc'):\n",
    "                pass\n",
    "            else:\n",
    "                p_attribution = []\n",
    "                for pheno in results:\n",
    "                    lat = pheno.latitude.item()\n",
    "                    lon = pheno.longitude.item()\n",
    "                    \n",
    "                    fi = regression_attribution(pheno,\n",
    "                                       X=dd.sel(latitude=lat, longitude=lon),\n",
    "                                       template=regress_template,\n",
    "                                       model_type=model_type,\n",
    "                                       pheno_var=regress_var,\n",
    "                                       modelling_vars=modelling_vars,\n",
    "                                      )\n",
    "                    p_attribution.append(fi)\n",
    "                \n",
    "                p_attribution = dask.compute(p_attribution)[0]\n",
    "                p_attribution = xr.combine_by_coords(p_attribution).astype('float32')\n",
    "                p_attribution.to_netcdf(f'{results_path}attribution_{regress_var}_{model_type}_perpixel_{n}.nc')\n",
    "            \n",
    "            # #do an extra one----------------------------------------------------------------------------------\n",
    "            # extra_template = xr.open_dataset(f'{template_path}template_PCMCI.nc').sel(feature=modelling_vars)\n",
    "            # if os.path.exists(f'{results_path}attribution_{regress_var}_PCMCI_perpixel_{n}.nc'):\n",
    "            #             pass\n",
    "            # else:\n",
    "            #     p_attribution = []\n",
    "            #     for pheno in results:\n",
    "            #         lat = pheno.latitude.item()\n",
    "            #         lon = pheno.longitude.item()\n",
    "                    \n",
    "            #         fi = regression_attribution(pheno,\n",
    "            #                            X=dd.sel(latitude=lat, longitude=lon),\n",
    "            #                            template=extra_template,\n",
    "            #                            model_type='PCMCI',\n",
    "            #                            pheno_var=regress_var,\n",
    "            #                            modelling_vars=modelling_vars,\n",
    "            #                           )\n",
    "            #         p_attribution.append(fi)\n",
    "                \n",
    "            #     p_attribution = dask.compute(p_attribution)[0]\n",
    "            #     p_attribution = xr.combine_by_coords(p_attribution).astype('float32')\n",
    "            #     p_attribution.to_netcdf(f'{results_path}attribution_{regress_var}_PCMCI_perpixel_{n}.nc')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d204a-37e5-4b67-b574-d32feccf3842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1d58d-9f1c-4ec6-bbe6-3fc0208d5297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece69c58-ab91-44d6-96e2-e3ad1c09fa97",
   "metadata": {},
   "source": [
    "## testing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f8e56-dbd3-4f57-8395-b4d543a0a286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64810b60-0aad-4e96-9025-f7d0a6726f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_attribution = xr.open_dataset(f'{results_path}attribution_{regress_var}_delta_slope_perpixel_{n}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6fd8f-0619-4fc2-bf07-d7c85224b39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _prediction import allNaN_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a7253-5e4b-41ee-92d5-fb3e0fb303bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,5,figsize=(18,4))\n",
    "for v,ax in zip(p_attribution.feature.values,axes.ravel()):\n",
    "    p_attribution.sel(feature=v).delta_slope.plot(add_labels=False,ax=ax,cmap='viridis')\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    ax.set_title(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95249bb-439d-4462-aae7-031ac3e35eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = allNaN_arg(np.abs(p_attribution.delta_slope), dim='feature',stat='max', idx=False)\n",
    "im = ss.plot(add_colorbar=False, figsize=(5,8), add_labels=False)\n",
    "cbar = fig.colorbar(im, ticks=[0,1,2,3,4], orientation='horizontal')\n",
    "cbar.ax.set_xticklabels(list(p_attribution.feature.values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb06e5-679a-4619-9a67-679c65ad8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corr = xr.open_dataset(f'{results_path}IOS_analysis_perpixel_{n}.nc')[['vPOS_parcorr', 'LOS_parcorr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74294f02-4fd5-438e-b6c5-0c34f1485286",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,2,figsize=(7,3))\n",
    "for v,ax in zip(p_corr.data_vars,axes.ravel()):\n",
    "    p_corr[v].plot(add_labels=False,ax=ax,cmap='viridis', vmin=0, vmax=1)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    ax.set_title(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4811-dc94-4ed1-9f56-d224091ef699",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(4,6))\n",
    "im = allNaN_arg(p_corr.to_array(), dim='variable',stat='max', idx=False).plot(ax=ax, add_colorbar=False)\n",
    "cbar = fig.colorbar(im, ticks=[0,1], orientation='horizontal')\n",
    "cbar.ax.set_xticklabels(list(p_corr.data_vars));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89f616-1b51-47ba-9213-2187bffa35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_trends.vPOS_slope.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b40a7-1636-41b9-9bba-b2ade993504f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618079bc-384c-4b55-9b11-c9ffb6ff7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # ### --------Handle NaNs---------\n",
    "        # # Due to issues with xarray quadratic interpolation, we need to remove\n",
    "        # # every NaN or else the daily interpolation function will fail\n",
    "        \n",
    "        # ##remove last ~6 timesteps that are all-NaN (from S-G smoothing).\n",
    "        # times_to_keep = d.mean(['latitude','longitude']).dropna(dim='time',how='any').time\n",
    "        # d = d.sel(time=times_to_keep)\n",
    "        \n",
    "        # #Find where NaNs are >10 % of data, will use this mask to remove pixels later.\n",
    "        # #and include any NaNs in the climate data.\n",
    "        # ndvi_nan_mask = np.isnan(d).sum('time') >= len(d.time) / 10\n",
    "        # clim_nan_mask = dd[['rain','vpd','tavg','srad']].to_array().isnull().any('variable')\n",
    "        # clim_nan_mask = (clim_nan_mask.sum('time')>0)\n",
    "        # nan_mask = (clim_nan_mask | ndvi_nan_mask)\n",
    "\n",
    "        # d = d.where(~nan_mask)\n",
    "        # dd = dd.where(~nan_mask)\n",
    "        # # nan_mask.to_netcdf(f'/g/data/os22/chad_tmp/Aus_phenology/data/ndvi_tiles/nan_mask_{n}.nc')\n",
    "        \n",
    "        # #fill the mostly all NaN slices with a fill value\n",
    "        # d = xr.where(nan_mask, -99, d)\n",
    "        \n",
    "        # #interpolate away any remaining NaNs\n",
    "        # d = d.interpolate_na(dim='time', method='cubic', fill_value=\"extrapolate\")\n",
    "        \n",
    "        # #now we can finally interpolate to daily\n",
    "        # d = d.resample(time='1D').interpolate(kind='quadratic').astype('float32')\n",
    "        \n",
    "        # # We also need the shape of the stacked array\n",
    "        # shape = d.stack(spatial=('latitude', 'longitude')).values.shape\n",
    "        \n",
    "        # #stack spatial indexes, this makes it easy to loop through data\n",
    "        # y_stack = d.stack(spatial=('latitude', 'longitude'))\n",
    "        # Y = y_stack.transpose('time', 'spatial')\n",
    "        \n",
    "        # # x_stack = dd.stack(spatial=('latitude', 'longitude'))\n",
    "        # # X = x_stack.transpose('time', 'spatial')\n",
    "        \n",
    "        # # find spatial indexes where values are mostly NaN (mostly land-sea mask)\n",
    "        # # This is where the nan_mask we created earlier = True\n",
    "        # idx_all_nan = np.where(nan_mask.stack(spatial=('latitude', 'longitude'))==True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
