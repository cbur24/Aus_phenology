{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6185d6-5015-4c74-a294-e3c3cea28081",
   "metadata": {},
   "source": [
    "# Determine causal drivers of trends in LSP metrics\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "**To Do:**\n",
    "* Determine best way to extract climate variables corresponding with phenology metrics - in this draft workflow I only extract mean climate over the preceeding two months of a given metric.\n",
    "* If using PLS, make maps of the coefficients for each variable.\n",
    "* Consider adding earlier metrics as a co-variable (e.g if modelling POS add SOS.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c7445-4b35-4c1a-b586-353f4a293c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import warnings\n",
    "import distinctipy\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from datetime import datetime\n",
    "import pymannkendall as mk\n",
    "from odc.geo.xr import assign_crs\n",
    "from odc.geo.geom import Geometry\n",
    "# from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "# from sklearn.linear_model import LinearRegression, TheilSenRegressor, BayesianRidge\n",
    "\n",
    "# import shap\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score #mean_squared_error\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "# from _utils import start_local_dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f402f2d-ab13-41f3-8442-43722664c9d5",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c27f5-4dbd-40e2-904d-961b23786d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# product='GIMMS-PKU'\n",
    "# product='AusENDVI-clim'\n",
    "product='AusENDVI-clim_MCD43A4'\n",
    "# product='GIMMS-PKU_2022'\n",
    "# product='GIMMSv1.1'\n",
    "# product='GIMMSv1.2'\n",
    "# product='MCD43A4'\n",
    "# product='Landsat'\n",
    "# product='AusENDVI-clim_2000'\n",
    "\n",
    "base = '/g/data/os22/chad_tmp/Aus_phenology/'\n",
    "\n",
    "ds_path = '/g/data/os22/chad_tmp/AusENDVI/results/publication/AusENDVI-clim_MCD43A4_gapfilled_1982_2022.nc'\n",
    "\n",
    "# timeseries_file = '/g/data/os22/chad_tmp/Aus_phenology/data/ecoregions_climate_timeseries.pkl'\n",
    "# timeseries_file ='/g/data/os22/chad_tmp/Aus_phenology/data/IBRA_regions_climate_timeseries.pkl'\n",
    "# timeseries_file =f'{base}data/pickle/IBRA_subregions_{product}_climate.pkl'\n",
    "timeseries_file =f'{base}data/pickle/IBRA_subregions_climate.pkl'\n",
    "\n",
    "# ecoregions_file = '/g/data/os22/chad_tmp/Aus_phenology/data/vectors/Ecoregions2017_modified.geojson'\n",
    "# ecoregions_file = '/g/data/os22/chad_tmp/Aus_phenology/data/vectors/IBRAv7_regions_modified.geojson'\n",
    "ecoregions_file = '/g/data/os22/chad_tmp/Aus_phenology/data/vectors/IBRAv7_subregions_modified.geojson'\n",
    "\n",
    "phenometrics_file = f'{base}data/pickle/IBRA_subregions_{product}_phenometrics.pkl'\n",
    "\n",
    "# var='ECO_NAME'\n",
    "# var='REG_NAME_7'\n",
    "var='SUB_NAME_7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6096af-0efe-4f0a-88d7-cc193cd512c2",
   "metadata": {},
   "source": [
    "## Open climate timeseries and phenometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c51f0-dcb2-4357-8501-976556a00068",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(timeseries_file, 'rb') as f:\n",
    "    climate = pickle.load(f)\n",
    "\n",
    "with open(phenometrics_file, 'rb') as f:\n",
    "    phenometrics = pickle.load(f)\n",
    "\n",
    "#open ecoregions\n",
    "gdf = gpd.read_file(ecoregions_file)\n",
    "#remove regions we dropped\n",
    "gdf = gdf[gdf[var].isin(list(phenometrics.keys()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a159dd-97e9-4105-a5f3-34717911d25f",
   "metadata": {},
   "source": [
    "## Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3b9bf-9b0d-4ef1-8f3a-bb07aeb6bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = []\n",
    "# for k in phenometrics.keys():\n",
    "#     df = phenometrics[k]\n",
    "\n",
    "#     #include a column counting the number of seasons\n",
    "#     df['n_seasons'] = len(df)\n",
    "    \n",
    "#     mean_df = df.quantile(q=0.5, interpolation='lower') # median\n",
    "#     mean_df = mean_df.rename({i:'mean_'+i for i in mean_df.index}).to_frame().transpose()\n",
    "#     mean_df[var] = k\n",
    "\n",
    "#     mean_gdf = gdf.merge(mean_df, on=var)\n",
    "#     means.append(mean_gdf)\n",
    "    \n",
    "# pheno_means = pd.concat(means).reset_index(drop=True)\n",
    "# pheno_means[['mean_POS', 'geometry', var]].explore(column='mean_POS', cmap='twilight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2311a-b15c-4366-8093-d4c94fdb799f",
   "metadata": {},
   "source": [
    "## Calculate climate metrics that relate to phenometrics\n",
    "\n",
    "Let's start with peak of season (POS) as thats an easy one.\n",
    "\n",
    "For each ecoregion, find the average date of POS, then we can calculate annual summary statistics around that POS date.  Let's do the two-months preceeding the POS, inclusive. So if the peak is in June, then we can calculate summary statistics from May-June...that'll be a good start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f21c7-4965-4978-9cd5-3281e92ecb98",
   "metadata": {},
   "source": [
    "#### IOS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c443e33-3896-41b9-be1b-fe18c137e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e045ee-0413-4385-8092-c28cefc079ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/g/data/os22/chad_tmp/Aus_phenology/src')\n",
    "# from phenology_pixel import pls_modelling\n",
    "\n",
    "# metrics_to_extract = ['IOS'] #'sos', 'eos', 'los'\n",
    "\n",
    "# for metric in metrics_to_extract:\n",
    "\n",
    "#     if not os.path.exists(f'{base}/data/{metric}/'):\n",
    "#         os.makedirs(f'{base}/data/{metric}/')\n",
    "    \n",
    "#     pheno_clim={}\n",
    "#     i=0\n",
    "#     for index, row in gdf.iterrows():\n",
    "#         print(metric+\" {:02}/{:02}\\r\".format(i + 1, len(range(0, len(gdf)))), end=\"\")\n",
    "#         print(row[var])\n",
    "#         #open corresponding phenometrics\n",
    "#         pheno = phenometrics[row[var]]\n",
    "\n",
    "#         covars = pls_modelling(pheno.to_xarray().expand_dims(latitude=[-33.0],longitude=[135.0]),\n",
    "#                                climate[row[var]], template=None, pheno_var='IOS',\n",
    "#                               modelling_vars=['CO2','srad', 'rain','tavg', 'vpd']\n",
    "#                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0c080-af26-45b7-ae15-518ccddec3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_extract = ['IOS'] #'sos', 'eos', 'los'\n",
    "\n",
    "for metric in metrics_to_extract:\n",
    "\n",
    "    if not os.path.exists(f'{base}/data/{metric}/'):\n",
    "        os.makedirs(f'{base}/data/{metric}/')\n",
    "    \n",
    "    pheno_clim={}\n",
    "    i=0\n",
    "    for index, row in gdf.iterrows():\n",
    "        print(metric+\" {:02}/{:02}\\r\".format(i + 1, len(range(0, len(gdf)))), end=\"\")\n",
    "    \n",
    "        #open corresponding phenometrics\n",
    "        pheno = phenometrics[row[var]]\n",
    "        \n",
    "        # Summarise climate data over length of season\n",
    "        # So we need the start time and end time and we'll summarise climate over\n",
    "        # that interval and append it to the pheno dataframe\n",
    "        start = [pd.Timestamp(datetime.strptime(f'{int(y)} {int(doy)}', '%Y %j')) for y,doy in zip(pheno.SOS_year, pheno.SOS)]\n",
    "        end = [pd.Timestamp(datetime.strptime(f'{int(y)} {int(doy)}', '%Y %j')) for y,doy in zip(pheno.EOS_year, pheno.EOS)]\n",
    "        \n",
    "        clim=[]\n",
    "        for s,e in zip(start,end):\n",
    "            c = climate[row[var]].sel(time=slice(s,e))\n",
    "            r = c['rain']\n",
    "            r = r.sum('time')\n",
    "            c = c[['CO2', 'srad', 'tavg', 'vpd', 'trees']]\n",
    "            c = c.mean('time')\n",
    "            c['rain'] = r\n",
    "            c = c.drop_vars('spatial_ref')\n",
    "            c = c.to_array().to_dataframe(name=e.year).transpose()\n",
    "            clim.append(c)\n",
    "        \n",
    "        covars = pd.concat(clim).reset_index(drop=True)\n",
    "        covars = covars.join(pheno['IOS'])\n",
    "        covars.index = pheno['POS_year'].values\n",
    "        covars.index.name = 'year'\n",
    "        \n",
    "        #add to results dict\n",
    "        pheno_clim[row[var]] = covars\n",
    "        i+=1\n",
    "\n",
    "    # #export model input data\n",
    "    for k,v in pheno_clim.items():\n",
    "        n=k.replace(\" \", \"_\")\n",
    "        pheno_clim[k].to_csv(f'{base}data/{metric}/{n}_{metric}_model_data.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822d047-6c89-4f85-bf56-0f2754ac1083",
   "metadata": {},
   "source": [
    "### vPOS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc042f2-e086-42ff-a641-8072fed4cb0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def months_filter(month, start, end):\n",
    "    return (month >= start) & (month <= end)\n",
    "\n",
    "metrics_to_extract = ['vPOS'] #'sos', 'eos', 'los'\n",
    "\n",
    "for metric in metrics_to_extract:\n",
    "\n",
    "    if not os.path.exists(f'{base}/{metric}/'):\n",
    "        os.makedirs(f'{base}/{metric}/')\n",
    "    \n",
    "    pheno_clim={}\n",
    "    i=0\n",
    "    for index, row in gdf.iterrows():\n",
    "        print(metric+\" {:02}/{:02}\\r\".format(i + 1, len(range(0, len(gdf)))), end=\"\")\n",
    "    \n",
    "        #open corresponding phenometrics\n",
    "        pheno = phenometrics[row[var]]\n",
    "            \n",
    "        #find average time for POS - just use a random year\n",
    "        # doys = [int(i) for i in pheno['pos_times'].values]\n",
    "        # years = [i for i in pheno.year.values]\n",
    "        # times = [pd.Timestamp(datetime.strptime(f'{y} {d}', '%Y %j')) for y, d in zip(years,doys)]\n",
    "        mean_pos = pd.Timestamp(datetime.strptime(f'{2000} {int(pheno.POS.mean())}', '%Y %j'))\n",
    "    \n",
    "        #subtract 2 months to find the month-range for summarising climate\n",
    "        months_before = mean_pos - pd.DateOffset(months=1)\n",
    "    \n",
    "        ##NEED TO IMPROVE THIS SO WHEN MONTHS GO FROM 12-2 FOR EXAMPLE IT STILL WORKS\n",
    "        # m_r = list(range(months_before.month, mean_pos.month+1, 1))\n",
    "        m_r = months_before.month, mean_pos.month\n",
    "        # print(m_r)\n",
    "    \n",
    "        #now we meed to index the climate data by the range of months\n",
    "        trimmed_climate = climate[row[var]].sel(time=months_filter(climate[row[var]]['time.month'],\n",
    "                                                    m_r[0], m_r[-1]))\n",
    "\n",
    "        # select years in climate only in years when peaks occur \n",
    "        # trimmed_climate = trimmed_climate.sel(year=pheno['POS_year'].values)\n",
    "        \n",
    "        #calculate annual climate summary stats\n",
    "        rain=trimmed_climate['rain']\n",
    "        rain=rain.groupby('time.year').sum()\n",
    "        trimmed_climate = trimmed_climate[['CO2', 'srad', 'tavg', 'vpd', 'trees']]\n",
    "        annual_climate = trimmed_climate.groupby('time.year').mean()\n",
    "        annual_climate = annual_climate.sel(year=pheno['POS_year'].values)\n",
    "        annual_climate['rain'] = rain.sel(year=pheno['POS_year'].values)\n",
    "    \n",
    "        # join our POS metric to the climate data after updating\n",
    "        # index of our metric with the years\n",
    "        pheno[f'{metric}'].index = pheno['POS_year'].values\n",
    "        pheno[f'{metric}'].index.name = 'year'\n",
    "        annual_climate[f'{metric}'] = pheno[f'{metric}']\n",
    "        \n",
    "        # if metric != 'los':\n",
    "        #     annual_climate[f'{metric}'] = pheno[f'{metric}']\n",
    "    \n",
    "        #add to results dict\n",
    "        pheno_clim[row[var]] = annual_climate\n",
    "        i+=1\n",
    "    # #export model input data\n",
    "    for k,v in pheno_clim.items():\n",
    "        n=k.replace(\" \", \"_\")\n",
    "        pheno_clim[k].drop_vars('spatial_ref').to_dataframe().to_csv(f'{base}data/{metric}/{n}_{metric}_model_data.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc410e-6847-436a-8c7e-ef7e660d0971",
   "metadata": {},
   "source": [
    "## Develop regressions models to predict change in phenometrics\n",
    "\n",
    "To do: extract 'trends' from each timeseries through decomposition and then regress the trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204c184-66ee-4eb6-b0cc-ac5acb3fd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/os22/chad_tmp/Aus_phenology/data/IOS/Southern_Ranges_IOS_model_data.csv'\n",
    "pheno_var= 'IOS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26e08b-41b8-4abf-b4d7-492ec680f65e",
   "metadata": {},
   "source": [
    "### Exploratory Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606f72a-e6e7-4eec-a56a-939e68f3f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, index_col='year')\n",
    "df = df.rolling(5, min_periods=5).mean()\n",
    "df = df.dropna()\n",
    "# df = df[['vPOS', 'pos_times', 'CO2', 'rain', 'srad', 'tavg', 'vpd']]\n",
    "df.plot(subplots=True, layout=(2,4), figsize=(14,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1a847-6165-4e38-92f7-a6bf72e6cac6",
   "metadata": {},
   "source": [
    "## Iterative linear modelling\n",
    "\n",
    "To determine the impact each variable has on predicting the slope of phenometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c42bf-4685-49c3-b857-97e8bcaa9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first fit a model with all vars\n",
    "x = df[['CO2','rain', 'tavg', 'vpd', 'srad', 'trees']]\n",
    "y = df[pheno_var]\n",
    "\n",
    "lr = PLSRegression().fit(x, y)\n",
    "# lr = BayesianRidge().fit(x, y)\n",
    "# lr = LinearRegression().fit(x, y)\n",
    "# lr = TheilSenRegressor().fit(x, y)\n",
    "prediction = lr.predict(x)\n",
    "r2_all = r2_score(y, prediction)\n",
    "\n",
    "#calculate slope of predicted variable with all params\n",
    "# s_actual, i, r, p, se = stats.linregress(df.index, y)\n",
    "result = mk.original_test(y, alpha=0.1) #\n",
    "s_actual = result.slope\n",
    "\n",
    "# s, i, r, p, se = stats.linregress(df.index, prediction)\n",
    "result_pred = mk.original_test(prediction, alpha=0.1) #\n",
    "s_prediction = result_pred.slope\n",
    "print(s_actual, s_prediction)\n",
    "print(lr.coef_)\n",
    "\n",
    "fig,ax=plt.subplots(1,1, figsize=(7,4))\n",
    "ax.plot(y.values, label='Observed')\n",
    "ax.plot(prediction, label='All')\n",
    "\n",
    "# now fit a model without a given variable\n",
    "# and calculate the slope of the phenometric\n",
    "r_delta={}\n",
    "s_delta={}\n",
    "for v in ['CO2','rain', 'tavg', 'vpd', 'srad', 'trees']:\n",
    "    #set variable as constant \n",
    "    constant = x[v].mean()\n",
    "    xx = x.drop(v, axis=1)\n",
    "    xx[v] = constant\n",
    "\n",
    "    #model and determine slope\n",
    "    # lr = TheilSenRegressor().fit(xx, y)\n",
    "    # lr = LinearRegression().fit(xx, y)\n",
    "    # lr = BayesianRidge().fit(xx, y)\n",
    "    lrr = PLSRegression().fit(xx, y)\n",
    "    pred = lrr.predict(xx)\n",
    "    r2 = r2_score(y, pred)\n",
    "    # s_p, i, r, p, se = stats.linregress(df.index, pred)\n",
    "    resulty = mk.original_test(pred, alpha=0.1) #\n",
    "    s_p = resulty.slope\n",
    "    \n",
    "    # plt.plot(y.values, label='Real')\n",
    "    ax.plot(pred, label=v)\n",
    "    #determine the eucliden distance between\n",
    "    #modelled slope and actual (and r2)\n",
    "    \n",
    "    s_delta[v] = math.dist((s_prediction,), (s_p,))\n",
    "    r_delta[v] = math.dist((r2_all,), (r2,))\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xticks(ticks = range(0,40, 4), labels=range(1982,2022, 4))\n",
    "ax.set_ylabel('vPOS (NDVI)')\n",
    "# ax.set_title('Jarrah-Karri Forest')\n",
    "\n",
    "s_delta = pd.Series(s_delta)\n",
    "r_delta = pd.Series(r_delta)\n",
    "sensivity = pd.concat([s_delta, r_delta], axis=1).rename({0:'Slope_difference', 1:'r2_difference'}, axis=1)\n",
    "sensivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46cb99-61d8-4ead-8456-2694aafc0c25",
   "metadata": {},
   "source": [
    "## Model causal drivers\n",
    "\n",
    "trying a couple of methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de3fd5-0fca-41d9-82fa-21f6fb4e436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/Aus_phenology/data/'\n",
    "metrics_to_extract = ['IOS'] #'sos', 'eos', 'los'\n",
    "pheno_var = 'IOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9868d-fb41-4e5e-926d-4c2be1489502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file('/g/data/os22/chad_tmp/Aus_phenology/data/Ecoregions2017_aus_processed.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1f03c-342f-41a2-ab76-8b71e09dd28b",
   "metadata": {},
   "source": [
    "### Using iterative linear modelling\n",
    "Delta-slope method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c264f-3250-426c-bc3f-a603fae1f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_to_extract:\n",
    "    files = [f for f in os.listdir(f'{base}{metric}/') if f.endswith('.csv') ]\n",
    "    files.sort()\n",
    "    print(metric)\n",
    "    \n",
    "    dffs = []\n",
    "    for f in files:\n",
    "        name = f.removesuffix('_'+metric+'_model_data.csv').replace(\"_\", \" \")\n",
    "        # print('', name)\n",
    "        \n",
    "        #open data\n",
    "        df = pd.read_csv(f'{base}{metric}/{f}', index_col='year')\n",
    "        # df = df[[metric, 'CO2', 'rain', 'srad', 'tavg', 'vpd']]\n",
    "        df = df.rolling(5, min_periods=5).mean()\n",
    "        df = df.dropna()\n",
    "        \n",
    "        #first fit a model with all vars\n",
    "        x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "        y = df[pheno_var]\n",
    "        lr = PLSRegression().fit(x, y)\n",
    "        prediction = lr.predict(x)\n",
    "        r2_all = r2_score(y, prediction)\n",
    "\n",
    "        # Find the robust slope of actual\n",
    "        result_actual = mk.original_test(y, alpha=0.1) #\n",
    "        p_actual = result_actual.p\n",
    "        s_actual = result_actual.slope\n",
    "        \n",
    "        #calculate slope of predicted variable with all params\n",
    "        result_prediction = mk.original_test(prediction, alpha=0.1) #\n",
    "        p_prediction = result_prediction.p\n",
    "        s_prediction = result_prediction.slope\n",
    "\n",
    "        # now fit a model without a given variable\n",
    "        # and calculate the slope of the phenometric\n",
    "        r_delta={}\n",
    "        s_delta={}\n",
    "        for v in ['CO2', 'rain', 'tavg', 'vpd', 'srad']:\n",
    "            #set variable of interest as a constant value \n",
    "            constant = x[v].mean()\n",
    "            xx = x.drop(v, axis=1)\n",
    "            xx[v] = constant\n",
    "        \n",
    "            #model and determine slope\n",
    "            lrr = PLSRegression().fit(xx, y)\n",
    "            pred = lrr.predict(xx)\n",
    "            r2 = r2_score(y, pred)\n",
    "            \n",
    "            result_p = mk.original_test(pred, alpha=0.1)\n",
    "            s_p = result_p.slope\n",
    "\n",
    "            #determine the eucliden distance between\n",
    "            #modelled slope and actual slope (and r2)\n",
    "            s_delta[v] = math.dist((s_prediction,), (s_p,))\n",
    "            r_delta[v] = math.dist((r2_all,), (r2,))\n",
    "\n",
    "        #determine most important feature\n",
    "        s_delta = pd.Series(s_delta)\n",
    "        r_delta = pd.Series(r_delta)\n",
    "        fi = pd.concat([s_delta, r_delta], axis=1).rename({0:'slope_difference', 1:'r2_difference'}, axis=1)\n",
    "        fi = fi.loc[[fi['slope_difference'].idxmax()]]\n",
    "        fi = fi.reset_index().rename({'index':'feature'},axis=1)\n",
    "\n",
    "        #create tidy df\n",
    "        fi[var] = name\n",
    "        fi['phenometric'] = pheno_var\n",
    "        fi['slope_actual'] = s_actual\n",
    "        fi['slope_modelled'] = s_prediction\n",
    "        fi['p_values'] = p_actual\n",
    "        fi['r2'] = r2_all\n",
    "        dffs.append(fi)\n",
    "    \n",
    "    top_features = pd.concat(dffs).reset_index(drop=True)\n",
    "    gdf_with_feature = gdf.merge(top_features, on=var)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa778ad-03a2-4e24-9cf9-b1618239be04",
   "metadata": {},
   "source": [
    "### Plot slope method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782c2d6-1b17-4851-82b1-3eca9e17f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = distinctipy.get_colors(len(x.columns),rng=4, pastel_factor=0.3, colorblind_type='Deuteranomaly')\n",
    "cmap = LinearSegmentedColormap.from_list('features', colors, N=len(x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c3eaf-7c72-4ed0-a632-7f16ea0bea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3, figsize=(18,5), sharey=True, layout='constrained')\n",
    "\n",
    "significant = gdf_with_feature[gdf_with_feature['p_values'] <= 1.0]\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[0])\n",
    "a=significant.plot(column='slope_actual', ax=ax[0], legend=True, cmap='BrBG', vmax=1,\n",
    "                   vmin=-1, legend_kwds={'shrink':0.8}) #  edgecolor=\"black\", linewidth=0.1 cmap='BrBG',\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[1])\n",
    "\n",
    "a=significant.plot(column='feature', ax=ax[1], legend=True, cmap=cmap) # \n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[2])\n",
    "a=significant.plot(column='slope_difference', ax=ax[2], legend=True,  vmin=0, vmax=0.75, cmap='magma', legend_kwds={'shrink':0.8})\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[0].set_title('vPOS Slope (NDVI/yr, p<=0.05) 1982-2022')\n",
    "ax[1].set_title('vPOS most important variable');\n",
    "ax[2].set_title('Sensitivity ('+u'Δslope'+')');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e62783-820b-4e46-b0b6-1ebe8af835f9",
   "metadata": {},
   "source": [
    "### Relationship between environmental gradients and CO2 sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423b715-1fd4-45a0-9947-690d7458f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = xr.open_dataarray('/g/data/os22/chad_tmp/Aus_phenology/data/MI_1982_2022.nc')\n",
    "mi = mi.mean('time')\n",
    "mi = assign_crs(mi, crs='EPSG:4326')\n",
    "mi = mi.rename('P:PET')\n",
    "mi = xr.where(mi>3, np.nan, mi) #remove extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3f526-a3a1-49e2-84f8-f8f7fa31c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for index,row in significant.iterrows():\n",
    "\n",
    "    #mask the wcf and mi datasets\n",
    "    geom = Geometry(geom=row.geometry, crs=significant.crs)\n",
    "    moist = mi.odc.mask(poly=geom)\n",
    "    moist = moist.mean(['latitude','longitude']).values.item()\n",
    "\n",
    "    results[row.SUB_NAME_7] = moist\n",
    "\n",
    "results = pd.Series(results,name='P:PET')\n",
    "significant = significant.join(results, on='SUB_NAME_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf5eba-8fac-4f79-8f20-bfac3f3eb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1, figsize=(5,4),  layout='constrained', sharey=True, sharex=True)\n",
    "\n",
    "obs,pred = significant['P:PET'].values, significant['slope_difference'].values\n",
    "mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "sl, i, r, p, s = stats.linregress(obs,pred)\n",
    "r2 = r**2\n",
    "\n",
    "xy = np.vstack([obs[mask],pred[mask]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=significant, x='P:PET',y='slope_difference', alpha=0.5, ax=ax, c=z)\n",
    "sb.regplot(data=significant, x='P:PET',y='slope_difference',  scatter=False,\n",
    "           line_kws={'linewidth':1}, color='black', ax=ax, robust=True)\n",
    "# ax.set_ylim(-0.006, 0.006)\n",
    "ax.set_title(u'Δslope'+' method')\n",
    "ax.set_ylabel('CO2 Sensitivity ('+u'Δslope'+')')\n",
    "# ax.axhline(0, color='grey', linestyle='--')\n",
    "ax.text(.70, .90, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(r2),\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "ax.text(.70, .825, 'p={:.3f}'.format(p), transform=ax.transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84bb31-1e2f-4d64-a69d-ea4f681f8377",
   "metadata": {},
   "source": [
    "### Partial least squares regression coefficients methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fd3c8-b90c-4179-a416-a54d90e8444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_to_extract:\n",
    "    files = [f for f in os.listdir(f'{base}{metric}/') if f.endswith('.csv') ]\n",
    "    files.sort()\n",
    "    print(metric)\n",
    "    \n",
    "    dffs = []\n",
    "    for f in files:\n",
    "        name = f.removesuffix('_'+metric+'_model_data.csv').replace(\"_\", \" \")\n",
    "        # print('', name)\n",
    "        \n",
    "        #open data\n",
    "        df = pd.read_csv(f'{base}{metric}/{f}', index_col='year')\n",
    "        # df = df[[metric, 'CO2', 'rain', 'srad', 'tavg', 'vpd']]\n",
    "        df = df.rolling(5, min_periods=5).mean()\n",
    "        df = df.dropna()\n",
    "        \n",
    "        #first fit a model with all vars\n",
    "        x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "        y = df[pheno_var]\n",
    "        lr = PLSRegression().fit(x, y)\n",
    "        prediction = lr.predict(x)\n",
    "        r2_all = r2_score(y, prediction)\n",
    "\n",
    "        # Find the robust slope of actual\n",
    "        result_actual = mk.original_test(y, alpha=0.1) #\n",
    "        p_actual = result_actual.p\n",
    "        s_actual = result_actual.slope\n",
    "        \n",
    "        #calculate slope of predicted variable with all params\n",
    "        result_prediction = mk.original_test(prediction, alpha=0.1) #\n",
    "        p_prediction = result_prediction.p\n",
    "        s_prediction = result_prediction.slope\n",
    "\n",
    "        fi = pd.Series(dict(zip(list(x.columns), list(lr.coef_.reshape(len(x.columns)))))).to_frame()\n",
    "        fi = fi.rename({0:'PLS_coefficent'},axis=1)\n",
    "        \n",
    "        # fi = pd.concat([s_delta, r_delta], axis=1).rename({0:'slope_difference', 1:'r2_difference'}, axis=1)\n",
    "        fi = fi.loc[[fi['PLS_coefficent'].abs().idxmax()]]\n",
    "        fi = fi.reset_index().rename({'index':'feature'},axis=1)\n",
    "\n",
    "        # #create tidy df\n",
    "        fi[var] = name\n",
    "        fi['phenometric'] = pheno_var\n",
    "        fi['slope_actual'] = s_actual\n",
    "        fi['slope_modelled'] = s_prediction\n",
    "        fi['p_values'] = p_actual\n",
    "        fi['r2'] = r2_all\n",
    "        dffs.append(fi)\n",
    "\n",
    "    top_features = pd.concat(dffs).reset_index(drop=True)\n",
    "    gdf_with_feature = gdf.merge(top_features, on=var)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4cb9f-a05e-4969-9ff2-7ea114818154",
   "metadata": {},
   "source": [
    "### Plot PLS coefficient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9984030-7c32-4d1e-8a3e-0fc1cc80d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3, figsize=(18,5), sharey=True, layout='constrained')\n",
    "\n",
    "significant = gdf_with_feature[gdf_with_feature['p_values'] <= 1.0]\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[0])\n",
    "a=significant.plot(column='slope_actual', ax=ax[0], legend=True, cmap='BrBG', vmax=0.002,\n",
    "                   vmin=-0.002, legend_kwds={'shrink':0.8}) #  edgecolor=\"black\", linewidth=0.1 cmap='BrBG',\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[1])\n",
    "\n",
    "# colors = distinctipy.get_colors(len(significant.feature.value_counts()),rng=1, colorblind_type='Deuteranomaly', pastel_factor=0.3)\n",
    "# cmap = LinearSegmentedColormap.from_list('features', colors, N=len(significant.feature.value_counts()))\n",
    "\n",
    "a=significant.plot(column='feature', ax=ax[1], legend=True, cmap=cmap)\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[2])\n",
    "a=significant.plot(column='PLS_coefficent', ax=ax[2], legend=True, vmin=-0.025, vmax=0.025, cmap='Spectral', legend_kwds={'shrink':0.8})\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[0].set_title('vPOS Slope (NDVI/yr, p<=0.0.05) 1982-2022')\n",
    "ax[1].set_title('vPOS most important variable');\n",
    "ax[2].set_title('Sensitivity (PLS coefficient)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea000b-1b92-477e-9be2-063d704e27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_with_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfacee6-54db-448f-9d98-8b3f77b9daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_significant = significant[significant['feature'] == 'CO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3fc19-e9ab-491d-a689-4a95f8becc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983c712-5d1b-4a14-b0cb-d8fcfdbae2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for index,row in co2_significant.iterrows():\n",
    "\n",
    "    #mask the wcf and mi datasets\n",
    "    geom = Geometry(geom=row.geometry, crs=co2_significant.crs)\n",
    "    moist = mi.odc.mask(poly=geom)\n",
    "    moist = moist.mean(['latitude','longitude']).values.item()\n",
    "\n",
    "    results[row.SUB_NAME_7] = moist\n",
    "\n",
    "results = pd.Series(results,name='P:PET')\n",
    "co2_significant = co2_significant.join(results, on='SUB_NAME_7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983829a6-467a-45a6-8749-2462406a96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1, figsize=(5,4),  layout='constrained', sharey=True, sharex=True)\n",
    "\n",
    "obs,pred = co2_significant['P:PET'].values, co2_significant['PLS_coefficent'].values\n",
    "mask = ~np.isnan(obs) & ~np.isnan(pred)\n",
    "sl, i, r, p, s = stats.linregress(obs,pred)\n",
    "r2 = r**2\n",
    "\n",
    "xy = np.vstack([obs[mask],pred[mask]])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=co2_significant, x='P:PET',y='PLS_coefficent', alpha=0.5, ax=ax, c=z)\n",
    "sb.regplot(data=co2_significant, x='P:PET',y='PLS_coefficent',  scatter=False,\n",
    "           line_kws={'linewidth':1}, color='black', ax=ax, robust=True)\n",
    "# ax.set_ylim(-0.006, 0.006)\n",
    "ax.set_title('PLS method')\n",
    "ax.set_ylabel('CO2 Sensitivity PLS method')\n",
    "# ax.axhline(0, color='grey', linestyle='--')\n",
    "ax.text(.70, .90, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(r2),\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "ax.text(.70, .825, 'p={:.3f}'.format(p), transform=ax.transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78878bc3-7ed1-4388-8e5e-bcad23460706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb3ecb-7f0d-433f-bad9-9210cbf890b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e74b9-e2a5-4b4a-990a-3bdef4abc855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724eb7b-2e19-44b0-8106-55a8437a0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(1,3, figsize=(18,5), sharey=True, layout='constrained')\n",
    "\n",
    "# significant = gdf_with_feature[gdf_with_feature['p_values'] <= 0.1]\n",
    "\n",
    "\n",
    "# gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[0])\n",
    "# a=significant.plot(column='slope_actual', ax=ax[0], legend=True, cmap='coolwarm', vmax=1.5,\n",
    "#                    vmin=-1.5, legend_kwds={'shrink':0.8}) #  edgecolor=\"black\", linewidth=0.1 cmap='BrBG',\n",
    "# ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "# gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[1])\n",
    "# cmap = ['tab:blue','tab:green','tab:orange','tab:red','tab:purple']\n",
    "# a=significant.plot(column='feature', ax=ax[1], legend=False, cmap=ListedColormap(cmap)) # \n",
    "# ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "# ax[1].legend(\n",
    "#         [Patch(facecolor=cmap[0]), Patch(facecolor=cmap[1]), Patch(facecolor=cmap[2]),Patch(facecolor=cmap[3]), Patch(facecolor=cmap[4])], \n",
    "#         ['CO2','rain', 'tavg', 'vpd', 'srad'],\n",
    "#          loc = 'upper right'\n",
    "#     );\n",
    "\n",
    "# gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[2])\n",
    "# a=significant.plot(column='slope_difference', ax=ax[2], legend=True,cmap='magma', vmin=0, vmax=1.5, legend_kwds={'shrink':0.8})\n",
    "# ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "# ax[0].set_title('POS Slope (days/yr, p<=0.1) 1982-2021')\n",
    "# ax[1].set_title('POS most important variable');\n",
    "# ax[2].set_title('Sensitivity ('+u'Δslope' + ')');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c31853d-d4c6-4fb0-9166-b23b7c428311",
   "metadata": {},
   "source": [
    "### Create a ML model\n",
    "\n",
    "And use Shap feature importance to determine most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64904d-2481-4ac1-8410-6348639d85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "# y = df[var]\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=300, random_state = 1).fit(x, y)\n",
    "# prediction = rf.predict(x)\n",
    "# # mse = mean_squared_error(y, prediction)\n",
    "# # rmse = mse**.5\n",
    "# # r2 = r2_score(y, prediction)\n",
    "# # print(r2)\n",
    "\n",
    "# # Lets plot the predictions versus the real values\n",
    "# plt.plot(y.values, label='Real')\n",
    "# plt.plot(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7d07e-1d62-452b-accb-e01e6f62020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature importance using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5d428-7751-4f4d-9cf7-a91c93df8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.Explainer(rf)\n",
    "# shap_values = explainer(x)\n",
    "\n",
    "# # fig, ax = plt.subplots(1,1, figsize=(5,7))\n",
    "# shap.plots.bar(shap_values, max_display=10, show=True)\n",
    "# # ax = plt.gca() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92779080-525e-4ede-9897-c0fc5ff9a711",
   "metadata": {},
   "source": [
    "## Test causal networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9aac5-6aaf-4d9b-a7dd-a2b5a31ff930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tigramite\n",
    "# from tigramite import data_processing as pp\n",
    "# from tigramite import plotting as tp\n",
    "# from tigramite.pcmci import PCMCI\n",
    "# from tigramite.independence_tests.parcorr import ParCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685e026-9aec-4b95-bc5d-6766ab4677f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['pos_values','CO2', 'rain', 'tavg', 'vpd','srad']]\n",
    "# data = df.values\n",
    "# T, N = data.shape\n",
    "# var_names = list(df.columns)\n",
    "\n",
    "# # Initialize dataframe object, specify time axis and variable names\n",
    "# dataframe = pp.DataFrame(data, \n",
    "#                          datatime = {0:np.arange(len(data))}, \n",
    "#                          var_names=var_names)\n",
    "\n",
    "# # tp.plot_timeseries(dataframe, figsize=(15, 5));\n",
    "\n",
    "# parcorr = ParCorr(significance='analytic')\n",
    "# pcmci = PCMCI(\n",
    "#     dataframe=dataframe, \n",
    "#     cond_ind_test=parcorr,\n",
    "#     verbosity=1)\n",
    "\n",
    "# correlations = pcmci.run_bivci(tau_max=1, val_only=True)['val_matrix']\n",
    "# matrix_lags = np.argmax(np.abs(correlations), axis=2)\n",
    "\n",
    "# # tp.plot_densityplots(dataframe=dataframe, setup_args={'figsize':(15, 10)}, add_densityplot_args={'matrix_lags':matrix_lags}); plt.show()\n",
    "\n",
    "# tau_max = 0\n",
    "# pc_alpha = 0.01\n",
    "# pcmci.verbosity = 0\n",
    "\n",
    "# results = pcmci.run_pcmciplus(tau_min=0, tau_max=tau_max, pc_alpha=pc_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283a581-a7e6-47e0-8f03-7bca3c369ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MCI partial correlations\")\n",
    "# print(results['val_matrix'].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec9a10-9487-41fc-a84e-0d7da75a7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp.plot_graph(\n",
    "#     figsize=(8,4),\n",
    "#     val_matrix=results['val_matrix'],\n",
    "#     graph=results['graph'],\n",
    "#     var_names=var_names,\n",
    "#     link_colorbar_label='cross-MCI (edges)',\n",
    "#     node_colorbar_label='auto-MCI (nodes)',\n",
    "#     ); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa9be3-df0d-458c-be0d-ee6337199c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae66afe-1291-421f-b3ee-6747f9ba350b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ff2c6-94e4-4416-b87f-4249c3f6ab47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63972f39-8769-42c8-81e1-ef6265fbca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncpus=10\n",
    "# def clim_timeseries(row, ds, results_dict):\n",
    "#     # clip to ecoregion\n",
    "#     geom = Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "#     yy = ds.odc.mask(poly=geom)\n",
    "    \n",
    "#     #summarise into 1d timeseries\n",
    "#     yy = yy.mean(['latitude', 'longitude'])\n",
    "\n",
    "#     results_dict[row.ECO_NAME] = yy\n",
    "\n",
    "# # parallel function for above function\n",
    "# def _parallel_fun(df, ds, ncpus):\n",
    "\n",
    "#     manager = mp.Manager()\n",
    "#     results_dict = manager.dict()\n",
    "\n",
    "#     # progress bar\n",
    "#     pbar = tqdm(total=len(gdf))\n",
    "\n",
    "#     def update(*a):\n",
    "#         pbar.update()\n",
    "\n",
    "#     with mp.Pool(ncpus) as pool:\n",
    "#         for index, row in df.iterrows():\n",
    "#             pool.apply_async(\n",
    "#                 clim_timeseries,\n",
    "#                 [row, ds, results_dict],\n",
    "#                 callback=update,\n",
    "#             )\n",
    "        \n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "#         pbar.close()\n",
    "\n",
    "#     return results_dict\n",
    "\n",
    "# # run the parallel function\n",
    "# results_clim = _parallel_fun(gdf, climate, ncpus=ncpus)\n",
    "# results_clim = results_clim._getvalue() #bring into memory\n",
    "\n",
    "# run the sequential function\n",
    "# results_dict={}\n",
    "# for index, row in gdf.iterrows():\n",
    "#     results_clim = clim_timeseries(row, climate, results_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
