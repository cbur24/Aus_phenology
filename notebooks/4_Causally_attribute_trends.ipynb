{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6185d6-5015-4c74-a294-e3c3cea28081",
   "metadata": {},
   "source": [
    "# (DRAFT) Define LSP metrics across Aus and determine causal drivers\n",
    "\n",
    "load NDVI data --> smooth --> remove non-seasonal regions --> summarise time-series over ecoregions --> calculate phenometrics --> load climate data --> summarise climate data over ecoregions --> select/summarise climate data into sensible metrics --> develop statistical relationships between phenometrics & climate (e.g. causal networks, partial correlations etc.)\n",
    "\n",
    "***\n",
    "\n",
    "**To Do:**\n",
    "* Split cropping class into rain-fed and irrigated classes (can be split it into cereal and pasture as well?)\n",
    "* ~~Remove \"Trobriand Islands rain forests\" and \"Sumba deciduous forests\", \"Timor and Wetar deciduous forests\" (in Indonesia) from eco-regions~~\n",
    "* ~~Multiprocess the climate variable extraction as its currently very slow running sequentially~~\n",
    "* Currently using defaults from phenolopy for extracting phenometrics - consider how we might best extract metrics (switching to R might be better for this). At least inspect the results for each ecoregion and assess phenolopy's ability to extract sensible phenology.\n",
    "* Use literature to determine best way to extract climate variables corresponding with phenology metrics - in this draft workflow I only extract mean climate over the preceeding two months of a given metric.\n",
    "* Consider sourcing climate data from somewhere other than ANUClim, perhaps ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c7445-4b35-4c1a-b586-353f4a293c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sb\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from odc.geo.xr import assign_crs\n",
    "from odc.geo.geom import Geometry\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import shap\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/Aus_phenology/src')\n",
    "# from xr_phenology import xr_phenology\n",
    "from phenolopy import calc_phenometrics\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools/')\n",
    "from dea_tools.spatial import xr_vectorize\n",
    "from dea_tools.classification import HiddenPrints\n",
    "\n",
    "# sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "# from _utils import start_local_dask\n",
    "# from _percentile import xr_quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f402f2d-ab13-41f3-8442-43722664c9d5",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c27f5-4dbd-40e2-904d-961b23786d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var='NDVI'\n",
    "ds_path = '/g/data/os22/chad_tmp/AusENDVI/results/publication/AusENDVI-clim_MCD43A4_gapfilled_1982_2022.nc'\n",
    "chunks=None#dict(latitude=1000, longitude=1000, time=-1)\n",
    "base = '/g/data/os22/chad_tmp/Aus_phenology/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d45f1-6dd9-44b4-bb31-825db4303234",
   "metadata": {},
   "source": [
    "## Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1bf71-29b3-4ea7-a9b9-0989f9bfb1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = assign_crs(xr.open_dataset(ds_path, chunks=chunks), crs='EPSG:4326')\n",
    "ds = ds.rename({'AusENDVI_clim_MCD43A4':'NDVI'})\n",
    "ds = ds['NDVI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812fff0-7dc6-4e37-8dcb-399de37db3ff",
   "metadata": {},
   "source": [
    "## Smoothing filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b282a-8a25-4655-acc0-63b8b758e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_up = ds.resample(time=\"2W\").interpolate(\"linear\")\n",
    "\n",
    "# # Savitsky-Golay\n",
    "ds_smooth = xr.apply_ufunc(\n",
    "        scipy.signal.savgol_filter,\n",
    "        ds_up,\n",
    "        input_core_dims=[['time']],\n",
    "        output_core_dims=[['time']],\n",
    "        kwargs=dict(\n",
    "            window_length=11,\n",
    "            polyorder=3,\n",
    "            deriv=0,\n",
    "            mode='interp'),\n",
    "        dask='allowed'\n",
    "    )\n",
    "#SG cuts of last 6 months so clip to complete calendar years.\n",
    "ds_smooth = ds_smooth.sel(time=slice('1982', '2021'))\n",
    "\n",
    "#smoothing reorders dims for some reason\n",
    "ds_smooth = ds_smooth.transpose('time', 'latitude','longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809784b-e3c7-43f3-89f7-1dee9f59aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1, figsize=(12,4))\n",
    "ds.mean(['latitude', 'longitude']).plot(ax=ax, label='NDVI')\n",
    "ds_smooth.mean(['latitude', 'longitude']).plot(ax=ax, label='NDVI Savitsky-Golay')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Signal smoothing of NDVI');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d6103-67c9-4b9a-a730-6d2a5e8b44ff",
   "metadata": {},
   "source": [
    "## Create a mask for non-seasonal regions\n",
    "\n",
    "To avoid applying phenology trend analysis on regions that do not experience regular seasonal variation, we will create a mask that removes areas designated as 'non-seasonal'. We use [Moore et al. 2016.](https://bg.copernicus.org/articles/13/5085/2016/) as a guide here (\"non-seasonally dynamic\": EVI varies but not in accordance with seasons), the thresholds needed to create the mask will be somewhat arbitrary but we will attempt to delineate areas with 1. low seasonal variability and low NDVI, and 2. areas with low seasonal variability and high interannual variability.\n",
    "\n",
    "Process:\n",
    "1. Calculate seasonally-adjusted anomalies\n",
    "2. Calcuate standard deviation in anomalies\n",
    "3. Overall mean NDVI\n",
    "4. Standard deviation of the mean seasonal pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f523d-feaf-4729-a77c-2ac583151ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "# #standardized anom\n",
    "# def anomalies(ds):\n",
    "#     return xr.apply_ufunc(\n",
    "#         lambda x, m: (x - m),\n",
    "#             ds.groupby(\"time.month\"),\n",
    "#             ds.groupby(\"time.month\").mean()\n",
    "#     )\n",
    "\n",
    "# # 3. Mean NDVI\n",
    "# mean_ndvi = ds_smooth.mean('time')\n",
    "\n",
    "# ##Mask very low NDVI (get rid of remnant water etc.)\n",
    "# low_ndvi_mask = xr.where(mean_ndvi<0.115,0, 1)\n",
    "# mean_ndvi = mean_ndvi.where(low_ndvi_mask)\n",
    "    \n",
    "# # 1. calculate anomalies\n",
    "# ndvi_std_anom = anomalies(ds_smooth)\n",
    "# # 2. std dev in anomalies\n",
    "# anom_std = ndvi_std_anom.std('time').where(low_ndvi_mask)\n",
    "# # 4. Std deviation of the mean seasonal pattern\n",
    "# std_mean_season = ds.groupby(\"time.month\").mean().std('month').where(low_ndvi_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac333c4-345b-4c02-a1b9-fa77503da164",
   "metadata": {},
   "source": [
    "### Create the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf88bb3-7309-49bb-b5fc-5fde2d361f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_seasonal_variability_low_ndvi = xr.where((std_mean_season<0.03) & (mean_ndvi<0.3), 1, 0)\n",
    "# low_seasonal_variability_high_anomaly = xr.where((std_mean_season<0.03) &  (anom_std>0.07), 1, 0)\n",
    "\n",
    "# seasonal_mask = xr.where((low_seasonal_variability_low_ndvi) | (low_seasonal_variability_high_anomaly), 0, 1)\n",
    "\n",
    "# #aply the mask\n",
    "# ds_smooth = ds_smooth.where(seasonal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9c299-dae1-43ad-a3db-a2db08a3b816",
   "metadata": {},
   "source": [
    "## Ecoregions & cropping regions\n",
    "\n",
    "From here: https://ecoregions.appspot.com/\n",
    "\n",
    "Ecoregions is natural ecologies only, we need to include a cropping class which we get from GFSAD\n",
    "\n",
    "> To Do: split cropping class into rain-fed and irrigated classes\n",
    "> Remove \"Trobriand Islands rain forests\" and \"Sumba deciduous forests\" (in Indonesia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea04301-b006-4d5b-95be-cc5f65dd4811",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/Aus_phenology/data/Ecoregions2017_aus_processed.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77dce53-54bf-46ea-8a55-3cd4d49ee8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #open ecoregions\n",
    "# gdf = gpd.read_file('/g/data/os22/chad_tmp/Aus_phenology/data/Ecoregions2017_aus.geojson')\n",
    "\n",
    "# # #clip to seasonal_mask\n",
    "# # mask_gdf = xr_vectorize(~seasonal_mask.astype('bool'), dtype='int16')\n",
    "# # mask_gdf = mask_gdf[mask_gdf['attribute']==0.0]\n",
    "# # mask_gdf['zone'] = 1\n",
    "# # mask_gdf = mask_gdf.dissolve(by='zone', aggfunc='sum')\n",
    "# # gdf = gpd.clip(gdf, mask_gdf)\n",
    "\n",
    "# #create a cropping geometery\n",
    "# crop = assign_crs(xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/data/1km/Landcover_1km_monthly_2002_2021.nc'), crs='epsg:4326').isel(time=1)\n",
    "# crop = (crop==40)\n",
    "# crop = xr_vectorize(crop, dtype='int16')\n",
    "# crop = crop[crop['attribute']==1.0]\n",
    "# crop['zone'] = 1\n",
    "# crop = crop.dissolve(by='zone', aggfunc='sum')\n",
    "\n",
    "# #remove areas in ecoregions where crops are\n",
    "# gdf = gpd.overlay(gdf, crop, how='symmetric_difference')\n",
    "\n",
    "# #add cropping polygons as a new geomtry to ecoregions\n",
    "# crop['ECO_NAME'] = 'Cropping' #add same attrs as eco\n",
    "# crop['BIOME_NAME'] = 'Cropping'\n",
    "# crop['BIOME_NUM'] = '0.0'\n",
    "# crop = crop.drop('attribute',axis=1).reset_index(drop=True)\n",
    "# gdf = pd.concat([gdf, crop]) #join\n",
    "# gdf = gdf.drop('attribute',axis=1).dropna(axis=0, how='any').reset_index(drop=True) #tidy\n",
    "# #remove tiny polys in Indo.\n",
    "# gdf = gdf[~gdf['ECO_NAME'].isin([\"Trobriand Islands rain forests\",\n",
    "#                                  \"Sumba deciduous forests\",\n",
    "#                                  \"Timor and Wetar deciduous forests\",\n",
    "#                                  \"Louisiade Archipelago rain forests\",\n",
    "#                                  \"New Guinea mangroves\",\n",
    "#                                 \"Southeast Papuan rain forests\",\n",
    "#                                 \"Timor and Wetar deciduous forests\"])].reset_index(drop=True)\n",
    "\n",
    "# gdf.to_file('/g/data/os22/chad_tmp/Aus_phenology/data/Ecoregions2017_aus_processed.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3d2ed-eb8f-4369-bd3c-a3b08b7852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gdf.plot(column='ECO_NAME', figsize=(8,8), legend=False, cmap='tab20b');\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "plt.title('Ecoregions', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9e13e-55cf-43eb-b2ef-dcd940b230fc",
   "metadata": {},
   "source": [
    "## Summarise NDVI & extract phenometrics over ecoregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041176ff-43c4-4399-ad14-c89719066cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "i=0\n",
    "for index, row in gdf.iterrows():\n",
    "    print(\"Feature {:02}/{:02}\\r\".format(i + 1, len(range(0, len(gdf)))), end=\"\")\n",
    "    \n",
    "    #clip to ecoregion\n",
    "    geom = Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    xx = ds_smooth.odc.mask(poly=geom)\n",
    "    \n",
    "    #summarise into 1d timeseries\n",
    "    xx = xx.mean(['latitude', 'longitude'])\n",
    "\n",
    "    #calculate phenometrics\n",
    "    \n",
    "    with HiddenPrints():\n",
    "        warnings.simplefilter('ignore')\n",
    "        phen = xx.groupby('time.year').map(calc_phenometrics)\n",
    "\n",
    "    #add to dict\n",
    "    results[row.ECO_NAME] = phen\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106a310-a8ce-41f6-98f3-ded43bf23855",
   "metadata": {},
   "source": [
    "## Test extraction of phenometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd7123-3d0d-471c-8753-b2be31eb9d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5356fc2-af25-4da2-8b8e-5cdfccff1607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6096af-0efe-4f0a-88d7-cc193cd512c2",
   "metadata": {},
   "source": [
    "## Climate data etc.\n",
    "\n",
    "load climate data --> summarise climate data over ecoregions --> select/summarise climate data into sensible metrics\n",
    "\n",
    "Consider getting climate from ERA5 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c51f0-dcb2-4357-8501-976556a00068",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clim = '/g/data/os22/chad_tmp/AusENDVI/data/5km/'\n",
    "co2 = xr.open_dataset(base_clim+'CO2_5km_monthly_1982_2022.nc')\n",
    "rain = xr.open_dataset(base_clim+'rain_5km_monthly_1981_2022.nc').sel(time=slice('1982','2022')).drop_vars('spatial_ref')\n",
    "srad = xr.open_dataset(base_clim+'srad_5km_monthly_1982_2022.nc').drop_vars('spatial_ref')\n",
    "tavg = xr.open_dataset(base_clim+'tavg_5km_monthly_1982_2022.nc').drop_vars('spatial_ref')\n",
    "vpd = xr.open_dataset(base_clim+'vpd_5km_monthly_1982_2022.nc').drop_vars('spatial_ref')\n",
    "# rain_cml3 = xr.open_dataset(base+'rain_cml3_5km_monthly_1982_2022.nc').drop_vars('spatial_ref')\n",
    "\n",
    "climate = xr.merge([co2, rain, srad, tavg, vpd])\n",
    "climate = assign_crs(climate, crs='EPSG:4326')\n",
    "climate = climate.transpose('time', 'latitude','longitude')\n",
    "\n",
    "climate = climate.sel(time=slice('1982', '2021')) #match trimmed NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35bf8e-f0c9-40bf-a484-56a2bd00d0dc",
   "metadata": {},
   "source": [
    "### Summarise climate data over polygons\n",
    "\n",
    "This is multiprocessed for faster processing. Enter `ncpus`\n",
    "\n",
    "Note, this will use way more memory than if run sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febf2bf-bec4-4969-a962-c59a388981cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63972f39-8769-42c8-81e1-ef6265fbca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_timeseries(row, ds, results_dict):\n",
    "    # clip to ecoregion\n",
    "    geom = Geometry(geom=row.geometry, crs=gdf.crs)\n",
    "    yy = ds.odc.mask(poly=geom)\n",
    "    \n",
    "    #summarise into 1d timeseries\n",
    "    yy = yy.mean(['latitude', 'longitude'])\n",
    "\n",
    "    results_dict[row.ECO_NAME] = yy\n",
    "\n",
    "# parallel function for above function\n",
    "def _parallel_fun(df, ds, ncpus):\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    results_dict = manager.dict()\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(total=len(gdf))\n",
    "\n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "\n",
    "    with mp.Pool(ncpus) as pool:\n",
    "        for index, row in df.iterrows():\n",
    "            pool.apply_async(\n",
    "                clim_timeseries,\n",
    "                [row, ds, results_dict],\n",
    "                callback=update,\n",
    "            )\n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pbar.close()\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "# run the parallel function\n",
    "results_clim = _parallel_fun(gdf, climate, ncpus=ncpus)\n",
    "results_clim = results_clim._getvalue() #bring into memory\n",
    "\n",
    "# run the sequential function\n",
    "# results_dict={}\n",
    "# for index, row in gdf.iterrows():\n",
    "#     results_clim = clim_timeseries(row, climate, results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2311a-b15c-4366-8093-d4c94fdb799f",
   "metadata": {},
   "source": [
    "### Calculate climate metrics that relate to phenometrics\n",
    "\n",
    "Let's start with peak of season (POS) as thats an easy one.\n",
    "\n",
    "For each ecoregion, find the average date of POS, then we can calculate annual summary statistics around they POS date.  Let's do the three-months preceeding the POS, inclusive. So if the peak is in June, then we can calculate summary statistics from April-May-June...that'll be a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc042f2-e086-42ff-a641-8072fed4cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_filter(month, start, end):\n",
    "    return (month >= start) & (month <= end)\n",
    "\n",
    "metrics_to_extract = ['pos'] #'sos', 'eos', 'los'\n",
    "\n",
    "for metric in metrics_to_extract:\n",
    "    print(metric)\n",
    "\n",
    "    if not os.path.exists(f'{base}/data/{metric}'):\n",
    "        os.makedirs(f'{base}/data/{metric}')\n",
    "    \n",
    "    results_pheno_clim={}\n",
    "    # i=0\n",
    "    for index, row in gdf.iterrows():\n",
    "        # print(\"Feature {:02}/{:02}\\r\".format(i + 1, len(range(0, len(gdf)))), end=\"\")\n",
    "    \n",
    "        #open corresponding phenometrics\n",
    "        pheno = results[row.ECO_NAME]\n",
    "    \n",
    "        #if all Nans then skip\n",
    "        if np.sum(np.isnan(pheno[f'{metric}_values'].values)) == len(pheno.year):\n",
    "            print('skip', row.ECO_NAME)\n",
    "            continue\n",
    "            \n",
    "        #find average time for POS - just use a random year\n",
    "        # doys = [int(i) for i in pheno['pos_times'].values]\n",
    "        # years = [i for i in pheno.year.values]\n",
    "        # times = [pd.Timestamp(datetime.strptime(f'{y} {d}', '%Y %j')) for y, d in zip(years,doys)]\n",
    "        mean_pos = pd.Timestamp(datetime.strptime(f'{2000} {int(pheno.pos_times.mean().values.item())}', '%Y %j'))\n",
    "    \n",
    "        #subtract 2 months to find the month-range for summarising climate\n",
    "        months_before = mean_pos - pd.DateOffset(months=1)\n",
    "    \n",
    "        ##NEED TO IMPROVE THIS SO WHEN MONTHS GO FROM 12-2 FOR EXAMPLE IT STILL WORKS\n",
    "        months_range = list(range(months_before.month, mean_pos.month+1, 1))\n",
    "        # print(months_range)\n",
    "    \n",
    "        #now we meed to index the climate data by the range of months\n",
    "        trimmed_climate = results_clim[row.ECO_NAME].sel(time=months_filter(results_clim[row.ECO_NAME]['time.month'],\n",
    "                                                                            months_range[0],months_range[-1]))\n",
    "    \n",
    "        #calculate annual climate summary stats (just mean for now)\n",
    "        rain=trimmed_climate['rain']\n",
    "        trimmed_climate = trimmed_climate[['CO2', 'srad', 'tavg', 'vpd']]\n",
    "        annual_climate = trimmed_climate.groupby('time.year').mean()\n",
    "        rain=rain.groupby('time.year').sum()\n",
    "        annual_climate['rain'] = rain\n",
    "    \n",
    "        #join our POS metric to the climate data\n",
    "        annual_climate[f'{metric}_values'] = pheno[f'{metric}_values']\n",
    "        if metric != 'los':\n",
    "            annual_climate[f'{metric}_times'] = pheno[f'{metric}_times']\n",
    "    \n",
    "        #add to results dict\n",
    "        results_pheno_clim[row.ECO_NAME] = annual_climate\n",
    "\n",
    "    # #export model input data\n",
    "    for k,v in results_pheno_clim.items():\n",
    "        n=k.replace(\" \", \"_\")\n",
    "        results_pheno_clim[k].drop('spatial_ref').to_dataframe().to_csv(f'{base}data/{metric}/{n}_{metric}_model_data.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc410e-6847-436a-8c7e-ef7e660d0971",
   "metadata": {},
   "source": [
    "## Develop basic model to predict change in phenometrics\n",
    "\n",
    "The goal here is to determine the most important features using shapely, this is one methods we might get to 'causality'\n",
    "\n",
    "Start with just a single phenometric and single location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204c184-66ee-4eb6-b0cc-ac5acb3fd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/os22/chad_tmp/Aus_phenology/data/pos/Jarrah-Karri_forest_and_shrublands_pos_model_data.csv'\n",
    "var= 'pos_values'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26e08b-41b8-4abf-b4d7-492ec680f65e",
   "metadata": {},
   "source": [
    "### Exploratory Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606f72a-e6e7-4eec-a56a-939e68f3f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, index_col='year')\n",
    "df = df.rolling(1, min_periods=1).mean()\n",
    "df = df.dropna()\n",
    "df = df[['pos_values', 'pos_times', 'CO2', 'rain', 'srad', 'tavg', 'vpd']]\n",
    "df.plot(subplots=True, layout=(2,4), figsize=(14,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bfd8f-33f6-4fad-ac69-646cb072a4ec",
   "metadata": {},
   "source": [
    "### Create a ML model\n",
    "\n",
    "And use Shap feature importance to determine most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64904d-2481-4ac1-8410-6348639d85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "# y = df[var]\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=300, random_state = 1).fit(x, y)\n",
    "# prediction = rf.predict(x)\n",
    "# # mse = mean_squared_error(y, prediction)\n",
    "# # rmse = mse**.5\n",
    "# # r2 = r2_score(y, prediction)\n",
    "# # print(r2)\n",
    "\n",
    "# # Lets plot the predictions versus the real values\n",
    "# plt.plot(y.values, label='Real')\n",
    "# plt.plot(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7d07e-1d62-452b-accb-e01e6f62020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature importance using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5d428-7751-4f4d-9cf7-a91c93df8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.Explainer(rf)\n",
    "# shap_values = explainer(x)\n",
    "\n",
    "# # fig, ax = plt.subplots(1,1, figsize=(5,7))\n",
    "# shap.plots.bar(shap_values, max_display=10, show=True)\n",
    "# # ax = plt.gca() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1a847-6165-4e38-92f7-a6bf72e6cac6",
   "metadata": {},
   "source": [
    "### Iterative linear modelling\n",
    "\n",
    "To determine the impact each variable has on predicting the slope of phenometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c42bf-4685-49c3-b857-97e8bcaa9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first fit a model with all vars\n",
    "x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "y = df[var]\n",
    "\n",
    "lr = LinearRegression().fit(x, y)\n",
    "prediction = lr.predict(x)\n",
    "r2_all = r2_score(y, prediction)\n",
    "\n",
    "#calculate slope of predicted variable with all params\n",
    "s_actual, i, r, p, se = stats.linregress(df.index, y)\n",
    "s, i, r, p, se = stats.linregress(df.index, prediction)\n",
    "print(s_actual, s)\n",
    "\n",
    "fig,ax=plt.subplots(1,1, figsize=(7,4))\n",
    "ax.plot(y.values, label='Observed')\n",
    "ax.plot(prediction, label='All')\n",
    "\n",
    "# now fit a model without a given variable\n",
    "# and calculate the slope of the phenometric\n",
    "r_delta={}\n",
    "s_delta={}\n",
    "for v in ['CO2','rain', 'tavg', 'vpd', 'srad']:\n",
    "    #set variable as constant \n",
    "    constant = x[v].mean()\n",
    "    xx = x.drop(v, axis=1)\n",
    "    xx[v] = constant\n",
    "\n",
    "    #model and determine slope\n",
    "    lr = LinearRegression().fit(xx, y)\n",
    "    pred = lr.predict(xx)\n",
    "    r2 = r2_score(y, pred)\n",
    "    s_p, i, r, p, se = stats.linregress(df.index, pred)\n",
    "    \n",
    "    # plt.plot(y.values, label='Real')\n",
    "    ax.plot(pred, label=v)\n",
    "    #determine the eucliden distance between\n",
    "    #modelled slope and actual (and r2)\n",
    "    s_delta[v] = math.dist((s,), (s_p,))\n",
    "    r_delta[v] = math.dist((r2_all,), (r2,))\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xticks(ticks = range(0,40, 4), labels=range(1982,2022, 4))\n",
    "ax.set_ylabel('vPOS (NDVI)')\n",
    "ax.set_title('Jarrah-Karri Forest')\n",
    "\n",
    "s_delta = pd.Series(s_delta)\n",
    "r_delta = pd.Series(r_delta)\n",
    "sensivity = pd.concat([s_delta, r_delta], axis=1).rename({0:'Slope_difference', 1:'r2_difference'}, axis=1)\n",
    "sensivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46cb99-61d8-4ead-8456-2694aafc0c25",
   "metadata": {},
   "source": [
    "## Loop through all regions and extract key variable\n",
    "\n",
    "Using iterative linear modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de3fd5-0fca-41d9-82fa-21f6fb4e436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/Aus_phenology/'\n",
    "metrics_to_extract = ['pos'] #'sos', 'eos', 'los'\n",
    "var = 'pos_values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9868d-fb41-4e5e-926d-4c2be1489502",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/Aus_phenology/data/Ecoregions2017_aus_processed.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c264f-3250-426c-bc3f-a603fae1f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_to_extract:\n",
    "    files = [f for f in os.listdir(f'{base}data/{metric}/') if f.endswith('.csv') ]\n",
    "    files.sort()\n",
    "    print(metric)\n",
    "    \n",
    "    dffs = []\n",
    "    for f in files:\n",
    "        name = f.removesuffix('_'+metric+'_model_data.csv').replace(\"_\", \" \")\n",
    "        # print('', name)\n",
    "        \n",
    "        #open data\n",
    "        df = pd.read_csv(f'{base}data/{metric}/{f}', index_col='year')\n",
    "        df = df[[metric+'_values', metric+'_times', 'CO2', 'rain', 'srad', 'tavg', 'vpd']]\n",
    "        df = df.dropna()\n",
    "        \n",
    "        #first fit a model with all vars\n",
    "        x = df[['CO2','rain', 'tavg', 'vpd', 'srad']]\n",
    "        y = df[var]\n",
    "        \n",
    "        lr = LinearRegression().fit(x, y)\n",
    "        prediction = lr.predict(x)\n",
    "        #calculate slope of predicted variable with all params\n",
    "        s_actual, i, r, p_actual, se = stats.linregress(df.index, y)\n",
    "        s_prediction, i, r, p, se = stats.linregress(df.index, prediction)\n",
    "        # print('  ', s_actual, s_prediction)\n",
    "        r2_all = r2_score(y, prediction)\n",
    "        \n",
    "        # now fit a model without a given variable\n",
    "        # and calculate the slope of the phenometric\n",
    "        r_delta={}\n",
    "        s_delta={}\n",
    "        for v in ['CO2','rain', 'tavg', 'vpd', 'srad']:\n",
    "            #set variable of interest as a constant value \n",
    "            constant = x[v].mean()\n",
    "            xx = x.drop(v, axis=1)\n",
    "            xx[v] = constant\n",
    "        \n",
    "            #model and determine slope\n",
    "            lrr = LinearRegression().fit(xx, y)\n",
    "            pred = lrr.predict(xx)\n",
    "            r2 = r2_score(y, pred)\n",
    "            s_p, i, r, p, se = stats.linregress(df.index, pred)\n",
    "\n",
    "            #determine the eucliden distance between\n",
    "            #modelled slope and actual slope (and r2)\n",
    "            s_delta[v] = math.dist((s_actual,), (s_p,))\n",
    "            r_delta[v] = math.dist((r2_all,), (r2,))\n",
    "\n",
    "        #determine most important feature\n",
    "        s_delta = pd.Series(s_delta)\n",
    "        r_delta = pd.Series(r_delta)\n",
    "        fi = pd.concat([s_delta, r_delta], axis=1).rename({0:'slope_difference', 1:'r2_difference'}, axis=1)\n",
    "        fi = fi.loc[[fi['slope_difference'].idxmax()]]\n",
    "        fi = fi.reset_index().rename({'index':'feature'},axis=1)\n",
    "\n",
    "        #create tidy df\n",
    "        fi['ECO_NAME'] = name\n",
    "        fi['phenometric'] = var\n",
    "        fi['slope_actual'] = s_actual\n",
    "        fi['slope_modelled'] = s_prediction\n",
    "        fi['p_values'] = p_actual\n",
    "        fi['r2'] = r2_all\n",
    "        dffs.append(fi)\n",
    "    \n",
    "    top_features = pd.concat(dffs).reset_index(drop=True)\n",
    "    gdf_with_feature = gdf.merge(top_features, on='ECO_NAME')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa778ad-03a2-4e24-9cf9-b1618239be04",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c3eaf-7c72-4ed0-a632-7f16ea0bea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3, figsize=(18,5), sharey=True, layout='constrained')\n",
    "\n",
    "significant = gdf_with_feature[gdf_with_feature['p_values'] <= 0.1]\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[0])\n",
    "a=significant.plot(column='slope_actual', ax=ax[0], legend=True, cmap='BrBG', vmax=0.002,\n",
    "                   vmin=-0.002, legend_kwds={'shrink':0.8}) #  edgecolor=\"black\", linewidth=0.1 cmap='BrBG',\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[1])\n",
    "cmap = ['tab:blue','tab:green','tab:orange','tab:red','tab:purple']\n",
    "a=significant.plot(column='feature', ax=ax[1], legend=False, cmap=ListedColormap(cmap)) # \n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[1].legend(\n",
    "        [Patch(facecolor=cmap[0]), Patch(facecolor=cmap[1]), Patch(facecolor=cmap[2]),Patch(facecolor=cmap[3]), Patch(facecolor=cmap[4])], \n",
    "        ['CO2','rain', 'tavg', 'vpd', 'srad'],\n",
    "         loc = 'upper right'\n",
    "    );\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[2])\n",
    "a=significant.plot(column='slope_difference', ax=ax[2], legend=True, vmin=0, vmax=0.0014, cmap='magma', legend_kwds={'shrink':0.8})\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[0].set_title('vPOS Slope (NDVI/yr, p<=0.1) 1982-2021')\n",
    "ax[1].set_title('vPOS most important variable');\n",
    "ax[2].set_title('Sensitivity ('+u'Δslope'+')');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724eb7b-2e19-44b0-8106-55a8437a0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,3, figsize=(18,5), sharey=True, layout='constrained')\n",
    "\n",
    "significant = gdf_with_feature[gdf_with_feature['p_values'] <= 0.1]\n",
    "\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[0])\n",
    "a=significant.plot(column='slope_actual', ax=ax[0], legend=True, cmap='coolwarm', vmax=1.5,\n",
    "                   vmin=-1.5, legend_kwds={'shrink':0.8}) #  edgecolor=\"black\", linewidth=0.1 cmap='BrBG',\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[1])\n",
    "cmap = ['tab:blue','tab:green','tab:orange','tab:red','tab:purple']\n",
    "a=significant.plot(column='feature', ax=ax[1], legend=False, cmap=ListedColormap(cmap)) # \n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[1].legend(\n",
    "        [Patch(facecolor=cmap[0]), Patch(facecolor=cmap[1]), Patch(facecolor=cmap[2]),Patch(facecolor=cmap[3]), Patch(facecolor=cmap[4])], \n",
    "        ['CO2','rain', 'tavg', 'vpd', 'srad'],\n",
    "         loc = 'upper right'\n",
    "    );\n",
    "\n",
    "gdf_with_feature.plot(edgecolor=\"black\", linewidth=0.05,facecolor='none', ax=ax[2])\n",
    "a=significant.plot(column='slope_difference', ax=ax[2], legend=True,cmap='magma', vmin=0, vmax=1.5, legend_kwds={'shrink':0.8})\n",
    "ctx.add_basemap(a, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "\n",
    "ax[0].set_title('POS Slope (days/yr, p<=0.1) 1982-2021')\n",
    "ax[1].set_title('POS most important variable');\n",
    "ax[2].set_title('Sensitivity ('+u'Δslope' + ')');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c435be-afc1-4925-acb8-718e92a707c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15a57c-10dc-4927-800d-04e651b5342d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92779080-525e-4ede-9897-c0fc5ff9a711",
   "metadata": {},
   "source": [
    "## Test causal networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9aac5-6aaf-4d9b-a7dd-a2b5a31ff930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests.parcorr import ParCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685e026-9aec-4b95-bc5d-6766ab4677f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['pos_values','CO2', 'rain', 'tavg', 'vpd','srad']]\n",
    "data = df.values\n",
    "T, N = data.shape\n",
    "var_names = list(df.columns)\n",
    "\n",
    "# Initialize dataframe object, specify time axis and variable names\n",
    "dataframe = pp.DataFrame(data, \n",
    "                         datatime = {0:np.arange(len(data))}, \n",
    "                         var_names=var_names)\n",
    "\n",
    "# tp.plot_timeseries(dataframe, figsize=(15, 5));\n",
    "\n",
    "parcorr = ParCorr(significance='analytic')\n",
    "pcmci = PCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=parcorr,\n",
    "    verbosity=1)\n",
    "\n",
    "correlations = pcmci.run_bivci(tau_max=1, val_only=True)['val_matrix']\n",
    "matrix_lags = np.argmax(np.abs(correlations), axis=2)\n",
    "\n",
    "# tp.plot_densityplots(dataframe=dataframe, setup_args={'figsize':(15, 10)}, add_densityplot_args={'matrix_lags':matrix_lags}); plt.show()\n",
    "\n",
    "tau_max = 0\n",
    "pc_alpha = 0.01\n",
    "pcmci.verbosity = 0\n",
    "\n",
    "results = pcmci.run_pcmciplus(tau_min=0, tau_max=tau_max, pc_alpha=pc_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283a581-a7e6-47e0-8f03-7bca3c369ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MCI partial correlations\")\n",
    "# print(results['val_matrix'].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec9a10-9487-41fc-a84e-0d7da75a7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp.plot_graph(\n",
    "#     figsize=(8,4),\n",
    "#     val_matrix=results['val_matrix'],\n",
    "#     graph=results['graph'],\n",
    "#     var_names=var_names,\n",
    "#     link_colorbar_label='cross-MCI (edges)',\n",
    "#     node_colorbar_label='auto-MCI (nodes)',\n",
    "#     ); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa9be3-df0d-458c-be0d-ee6337199c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae66afe-1291-421f-b3ee-6747f9ba350b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
