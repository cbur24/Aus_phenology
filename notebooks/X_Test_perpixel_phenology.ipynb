{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b43d99-d698-40b4-a6cb-1e205a6cc0d7",
   "metadata": {},
   "source": [
    "\n",
    "**To do**\n",
    "\n",
    "* ~~Handle all NaN case for land-sea.~~\n",
    "* ~~Test on interpolated daily NDVI values.~~\n",
    "* ~~Implement trends/slopes~~\n",
    "* Have answer exported as netcdf given long processing time.\n",
    "* Consider breaking Aus into six tiles and run these seperately so each run only takes a few hours. This could prevent the situation where several thousand SU are used only for the process to crash half way through\n",
    "\n",
    "<!-- \n",
    "```\n",
    "#performing regression\n",
    "a = np.zeros((72,144,3))\n",
    "for i in range(len(data.lat)):\n",
    "    for j in range(len(data.lon)):\n",
    "        a[i,j,:] = (Ridge().fit(np.array((dataU.isel(lev=2).x1.values[:,i,j].reshape(-1,1),\n",
    "                    dataU.isel(lev=2).x2.values[:,i,j].reshape(-1,1),\n",
    "                    dataU.isel(lev=2).x3.values[:,i,j].reshape(-1,1))).reshape(108,3),\n",
    "                    data2U.y.values[:,i,j].reshape(108)).coef_)\n",
    "dataU = data.assign_coords(varname=['x1','x2','x3'])\n",
    "dataU['multiple_reg_coeff'] = (('lat','lon','varname'), a)\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a23e7-5a01-4839-a9c2-f02880a81e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import dask\n",
    "import dask.array\n",
    "from dask import delayed\n",
    "\n",
    "import seaborn as sb\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from odc.geo.xr import assign_crs\n",
    "# from scipy.stats import gaussian_kde\n",
    "# from sklearn.metrics import r2_score\n",
    "import pymannkendall as mk\n",
    "# from xarrayMannKendall import Mann_Kendall_test\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304396c-c266-48cd-a707-e1dbca45d307",
   "metadata": {},
   "source": [
    "## Dask cluster\n",
    "\n",
    "Local or Dynamic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6a5f6-5bea-48af-8b7f-1ceaab3b7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '-q normal','-P w97','-l ncpus='+str(cores),'-l mem='+str(memory), '-l storage=gdata/os22+gdata/w97'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9803790-85dd-4e15-b3aa-0761ed7df361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.config\n",
    "# from dask.distributed import Client,LocalCluster\n",
    "# from dask_jobqueue import PBSCluster\n",
    "# walltime = '01:00:00'\n",
    "# cores = 24\n",
    "# memory = '50GB'\n",
    "\n",
    "# cluster = PBSCluster(walltime=str(walltime), cores=cores, memory=str(memory),processes=cores,\n",
    "#                      job_extra_directives=['-q normal','-P w97','-l ncpus='+str(cores),'-l mem='+str(memory),\n",
    "#                                 '-l storage=gdata/os22+gdata/w97'],\n",
    "#                      local_directory='$TMPDIR',\n",
    "#                      job_directives_skip=[\"select\"],\n",
    "#                      # python=os.environ[\"DASK_PYTHON\"]\n",
    "# #                     )\n",
    "# cluster.scale(jobs=2)\n",
    "# client = Client(cluster)\n",
    "\n",
    "from _utils import start_local_dask\n",
    "start_local_dask(n_workers=24, threads_per_worker=1, memory_limit='93GiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a97503-78ed-4316-b7b7-b1e4d7662e09",
   "metadata": {},
   "source": [
    "## Open datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe6d8d-3ff9-4e2e-82f8-046cb068f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_dataset('/g/data/os22/chad_tmp/Aus_phenology/data/NDVI/NDVI_smooth_AusENDVI-clim_MCD43A4.nc')['NDVI']\n",
    "# ds = ds.isel(latitude=slice(200,250), longitude=slice(200,250)) #testing sample\n",
    "# # ds = ds.dropna(dim='time',\n",
    "# #             how='all').resample(time='1W').interpolate(kind='quadratic')\n",
    "# # ds = assign_crs(ds, crs='EPSG:4326')\n",
    "# ds = ds.rename('NDVI')\n",
    "\n",
    "# covars = xr.open_dataset('/g/data/os22/chad_tmp/Aus_phenology/data/covars.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c85896-5100-41ac-a7e3-43d11e948099",
   "metadata": {},
   "source": [
    "## Per pixel phenometrics with dask.delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5a649-0ded-47f7-becb-5eddf37e35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_peaks_troughs(da,\n",
    "                      rolling=90,\n",
    "                      distance=90,\n",
    "                      prominence='auto',\n",
    "                      plateau_size=10\n",
    "                     ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Identifying peaks and troughs in a vegetation index time series\n",
    "    \n",
    "    The algorithm builds upon those described by Broich et al. (2014).  \n",
    "    The steps are as follows:\n",
    "    \n",
    "    1. Calculate rolling minimums\n",
    "    2. Calculate rolling maximums\n",
    "    3. Using scipy.signal.find_peaks, identify peaks and troughs in rolling max\n",
    "        and min time-series using a minimum distance between peaks, a peak plateau size,\n",
    "        and the peak/trough prominence.\n",
    "    4. Remove peaks or troughs where troughs (peaks) occur sequentially without\n",
    "       a peak (trough) between them. So enforce the pattern peak-valley-peak-valley etc.\n",
    "       This is achieved by taking the maximum (minimum) peak (trough) where two peaks (troughs)\n",
    "       occur sequentially.\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    A dictionary with keys the same as the input and the values a pandas.dataframe with peaks\n",
    "    and trough identified by the time stamp they occur.\n",
    "    \n",
    "    \"\"\"\n",
    "    #check its an array\n",
    "    if not isinstance(da, xr.DataArray):\n",
    "        raise TypeError(\n",
    "            \"This function only excepts an xr.DataArray\"\n",
    "        )\n",
    "    #doesn't matter what we call the variable just\n",
    "    #need it to be predicatable\n",
    "    # da = Y.isel(spatial=i)\n",
    "    da.name = 'NDVI'\n",
    "    \n",
    "    #ensure ds has only time coordinates\n",
    "    coords = list(da.coords)\n",
    "    coords.remove('time')\n",
    "    da = da.drop_vars(coords)\n",
    "    \n",
    "    #calculate rolling min/max to find local minima maxima\n",
    "    roll_max = da.rolling(time=rolling, min_periods=1, center=True).max()\n",
    "    roll_min = da.rolling(time=rolling, min_periods=1, center=True).min()\n",
    "    \n",
    "    if prominence=='auto':\n",
    "        #dynamically determine how prominent a peak needs to be\n",
    "        #based upon typical range of seasonal cycle\n",
    "        clim = da.groupby('time.month').mean()\n",
    "        _range = (clim.max() - clim.min()).values.item()\n",
    "        if _range>=0.05:\n",
    "            prominence = 0.01\n",
    "        if _range<0.05:\n",
    "            prominence = 0.005\n",
    "    \n",
    "    #find peaks and valleys\n",
    "    peaks = scipy.signal.find_peaks(roll_max.data,\n",
    "                        distance=distance,\n",
    "                        prominence=prominence,\n",
    "                        plateau_size=plateau_size)[0]\n",
    "    \n",
    "    troughs = scipy.signal.find_peaks(roll_min.data*-1,#invert\n",
    "                        distance=distance,\n",
    "                        prominence=prominence,\n",
    "                        plateau_size=plateau_size)[0]\n",
    "    \n",
    "    #--------------cleaning-------\n",
    "    # Identify where two peaks or two valleys occur one after another and remove.\n",
    "    # i.e. enforcing the pattern peak-vally-peak-valleys etc.\n",
    "    # First get the peaks and troughs into a dataframe with matching time index\n",
    "    df = da.to_dataframe()\n",
    "    df['peaks'] = da.isel(time=peaks).to_dataframe()\n",
    "    df['troughs'] = da.isel(time=troughs).to_dataframe()\n",
    "    df_peaks_troughs = df.drop('NDVI', axis=1).dropna(how='all')\n",
    "    \n",
    "    #find where two peaks or two troughs occur sequentially\n",
    "    peaks_num_nans = df_peaks_troughs.peaks.isnull().rolling(2).sum()\n",
    "    troughs_num_nans = df_peaks_troughs.troughs.isnull().rolling(2).sum()\n",
    "    \n",
    "    # Grab the indices where the rolling sum of NaNs equals 2.\n",
    "    # The labelling is inverted here because two NaNs in the trough column\n",
    "    # mean two peaks occur concurrently, and vice versa\n",
    "    idx_consecutive_peaks = troughs_num_nans[troughs_num_nans==2.0]\n",
    "    idx_consecutive_troughs = peaks_num_nans[peaks_num_nans==2.0]\n",
    "    \n",
    "    # Loop through locations with two sequential peaks and drop\n",
    "    # the smaller of the two peaks\n",
    "    for idx in idx_consecutive_peaks.index:\n",
    "        \n",
    "        loc = df_peaks_troughs.index.get_loc(idx)\n",
    "        df = df_peaks_troughs.iloc[[loc-1,loc]]\n",
    "        \n",
    "        min_peak_to_drop = df.idxmin(skipna=True).peaks\n",
    "        df_peaks_troughs = df_peaks_troughs.drop(min_peak_to_drop)\n",
    "    \n",
    "    # Loop through locations with two sequential troughs and drop\n",
    "    # the higher of the two troughs (less prominent trough)\n",
    "    for idx in idx_consecutive_troughs.index:\n",
    "        \n",
    "        loc = df_peaks_troughs.index.get_loc(idx)\n",
    "        df = df_peaks_troughs.iloc[[loc-1,loc]]\n",
    "        \n",
    "        min_trough_to_drop = df.idxmax(skipna=True).troughs\n",
    "        df_peaks_troughs = df_peaks_troughs.drop(min_trough_to_drop)\n",
    "    \n",
    "    return df_peaks_troughs\n",
    "\n",
    "@dask.delayed\n",
    "def phenometrics(da,\n",
    "             rolling=90,\n",
    "             distance=90,\n",
    "             prominence='auto',\n",
    "             plateau_size=10,\n",
    "             amplitude=0.20,  \n",
    "             verbose=True\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Calculate statistics that describe the phenology cycle of\n",
    "    a vegetation condition time series.\n",
    "    \n",
    "    Identifies the start and end points of each cycle using \n",
    "    the `seasonal amplitude` method. When the vegetation time series reaches\n",
    "    20% of the sesonal amplitude between the first minimum and the peak,\n",
    "    and the peak and the second minimum.\n",
    "    \n",
    "    To ensure we are measuring only full cycles we enforce the time series to\n",
    "    start and end with a trough.\n",
    "    \n",
    "    Phenometrics calculated:\n",
    "        * ``'SOS'``: DOY of start of season\n",
    "        * ``'POS'``: DOY of peak of season\n",
    "        * ``'EOS'``: DOY of end of season\n",
    "        * ``'vSOS'``: Value at start of season\n",
    "        * ``'vPOS'``: Value at peak of season\n",
    "        * ``'vEOS'``: Value at end of season\n",
    "        * ``'TOS'``: DOY of the minimum at the beginning of cycle (left of peak)\n",
    "        * ``'vTOS'``: Value at the beginning of cycle (left of peak)\n",
    "        * ``'LOS'``: Length of season (DOY)\n",
    "        * ``'AOS'``: Amplitude of season (in value units)\n",
    "        * ``'IOS'``: Integral of season (in value units)\n",
    "        * ``'ROG'``: Rate of greening (value units per day)\n",
    "        * ``'ROS'``: Rate of senescence (value units per day)\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    Dictionary where keys are the labels of the polygons, and values\n",
    "    are Pandas.Dataframe containing phenometrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Extract peaks and troughs in the timeseries\n",
    "    peaks_troughs = extract_peaks_troughs(\n",
    "                         da,\n",
    "                         rolling=rolling,\n",
    "                         distance=distance,\n",
    "                         prominence=prominence,\n",
    "                         plateau_size=plateau_size)\n",
    "    \n",
    "    # start the timeseries with trough\n",
    "    if np.isnan(peaks_troughs.iloc[0].troughs):\n",
    "        p_t = peaks_troughs.iloc[1:]\n",
    "    else:\n",
    "        p_t = peaks_troughs\n",
    "    \n",
    "    # end the timeseries with trough\n",
    "    if np.isnan(p_t.iloc[-1].troughs)==True:\n",
    "        p_t = p_t.iloc[0:-1]\n",
    "        \n",
    "    # Store phenology stats\n",
    "    pheno = {}\n",
    "    \n",
    "    peaks_only = p_t.peaks.dropna()\n",
    "    for peaks, idx in zip(peaks_only.index, range(0,len(peaks_only))):\n",
    "        # First we extract the trough times either side of the peak\n",
    "        start_time = p_t.iloc[p_t.index.get_loc(peaks)-1].name\n",
    "        end_time = p_t.iloc[p_t.index.get_loc(peaks)+1].name\n",
    "    \n",
    "        # now extract the NDVI time series for the cycle\n",
    "        ndvi_cycle = da.sel(time=slice(start_time, end_time))\n",
    "        \n",
    "        # add the stats to this\n",
    "        vars = {}\n",
    "       \n",
    "        # --Extract phenometrics---------------------------------\n",
    "        pos = ndvi_cycle.idxmax(skipna=True)\n",
    "        vars['POS_year'] = pos.dt.year #so we can keep track\n",
    "        vars['POS'] = pos.dt.dayofyear.values\n",
    "        vars['vPOS'] = ndvi_cycle.max().values\n",
    "        #we want the trough values from the beginning of the season only (left side)\n",
    "        vars['TOS_year'] =  p_t.iloc[p_t.index.get_loc(peaks)-1].name.year\n",
    "        vars['TOS'] = p_t.iloc[p_t.index.get_loc(peaks)-1].name.dayofyear\n",
    "        vars['vTOS'] = p_t.iloc[p_t.index.get_loc(peaks)-1].troughs\n",
    "        vars['AOS'] = (vars['vPOS'] - vars['vTOS'])\n",
    "        \n",
    "        #SOS ------ \n",
    "        # Find the greening cycle (left of the POS)\n",
    "        greenup = ndvi_cycle.where(ndvi_cycle.time <= pos)\n",
    "        # Find absolute distance between 20% of the AOS and the values of the greenup, then\n",
    "        # find the NDVI value that's closest to 20% of AOS, this is our SOS date\n",
    "        sos = np.abs(greenup - (vars['AOS'] * amplitude + vars['vTOS'])).idxmin(skipna=True)\n",
    "        vars['SOS_year'] = sos.dt.year #so we can keep track\n",
    "        vars['SOS'] = sos.dt.dayofyear.values\n",
    "        vars['vSOS'] = ndvi_cycle.sel(time=sos).values\n",
    "        \n",
    "        #EOS ------\n",
    "        # Find the senescing cycle (right of the POS)\n",
    "        browning = ndvi_cycle.where(ndvi_cycle.time >= ndvi_cycle.idxmax(skipna=True))\n",
    "        # Find absolute distance between 20% of the AOS and the values of the browning, then\n",
    "        # find the NDVI value that's closest to 20% of AOS, this is our EOS date\n",
    "        ampl_browning = browning.max() - browning.min()\n",
    "        eos = np.abs(browning - (ampl_browning * amplitude + browning.min())).idxmin(skipna=True)\n",
    "        vars['EOS_year'] = eos.dt.year #so we can keep track\n",
    "        vars['EOS'] = eos.dt.dayofyear.values\n",
    "        vars['vEOS'] = ndvi_cycle.sel(time=eos).values\n",
    "    \n",
    "        # LOS ---\n",
    "        los = (pd.to_datetime(eos.values) - pd.to_datetime(sos.values)).days\n",
    "        vars['LOS'] = los\n",
    "    \n",
    "        #Integral of season\n",
    "        ios = ndvi_cycle.sel(time=slice(sos, eos))\n",
    "        ios = ios.integrate(coord='time', datetime_unit='D')\n",
    "        vars['IOS'] = ios\n",
    "    \n",
    "        # Rate of growth and sensecing (NDVI per day)\n",
    "        vars['ROG'] = (vars['vPOS'] - vars['vSOS']) / ((pd.to_datetime(pos.values) - pd.to_datetime(sos.values)).days)\n",
    "        vars['ROS'] = (vars['vEOS'] - vars['vPOS']) / ((pd.to_datetime(eos.values) - pd.to_datetime(pos.values)).days) \n",
    "        # print(vars)\n",
    "        pheno[idx] = vars\n",
    "    \n",
    "    ds = pd.DataFrame(pheno).astype('float32').transpose().to_xarray()\n",
    "    \n",
    "    ds = ds.astype(np.float32)\n",
    "    lat = da.latitude.item()\n",
    "    lon = da.longitude.item()\n",
    "    ds.assign_coords(latitude=lat, longitude=lon)\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        ds[var] = ds[var].expand_dims(latitude=[lat], longitude = [lon])\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8c0e5-15f4-402c-a565-12599e000b21",
   "metadata": {},
   "source": [
    "### Handle NaNs\n",
    "Due to issues with xarray quadratic interpolation, we need to remove every NaN or else the daily interpolation function will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f7b48-42c7-4799-bbd6-a614475177ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/g/data/os22/chad_tmp/Aus_phenology/data/NDVI/NDVI_smooth_AusENDVI-clim_MCD43A4.nc')['NDVI']\n",
    "ds = ds.isel(latitude=slice(175,250), longitude=slice(175,250))\n",
    "\n",
    "##remove last ~6 timesteps that are all-NaN (from S-G smoothing).\n",
    "times_to_keep = ds.mean(['latitude','longitude']).dropna(dim='time',how='any').time\n",
    "ds = ds.sel(time=times_to_keep)\n",
    "\n",
    "#Find where NaNs are >10 % of data, will use this mask to remove pixels later.\n",
    "nan_mask = np.isnan(ds).sum('time') >= len(ds.time) / 10\n",
    "\n",
    "#fill the mostly all NaN slices with a fill value\n",
    "ds = xr.where(nan_mask, -99, ds)\n",
    "\n",
    "#interpolate away any remaining NaNs\n",
    "ds = ds.interpolate_na(dim='time', method='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "#now we can finally interpolate to daily\n",
    "ds = ds.resample(time='1D').interpolate(kind='quadratic')\n",
    "\n",
    "#export so we can test fnction\n",
    "ds.to_netcdf('/g/data/os22/chad_tmp/Aus_phenology/data/ndvi_test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecda0f1-ea69-4108-905d-6e46e7ba94c0",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb03eb-5edc-4397-b71d-db1f7d973d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open NDVI data\n",
    "path='/g/data/os22/chad_tmp/Aus_phenology/data/ndvi_test.nc'\n",
    "ds = xr.open_dataarray(path)\n",
    "\n",
    "#stack spatial indexes, this makes it easy to loop through data\n",
    "y_stack = ds.stack(spatial=('latitude', 'longitude'))\n",
    "Y = y_stack.transpose('time', 'spatial')\n",
    "\n",
    "#find spatial indexes where values are mostly NaN (mostly land-sea mask)\n",
    "# This is where the nan_mask we created earlier == True\n",
    "idx_all_nan = np.where(nan_mask.stack(spatial=('latitude', 'longitude'))==True)[0]\n",
    "\n",
    "# open template array which we'll use \n",
    "# whenever we encounter an all-NaN index.\n",
    "# Created the template using one of the output results\n",
    "# bb = xr.full_like(results[0], fill_value=-99, dtype='float32')\n",
    "template_path='/g/data/os22/chad_tmp/Aus_phenology/data/template.nc'\n",
    "ds_template = xr.open_dataset(template_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db09dd-2fd7-42a3-b18b-f8307fd9f321",
   "metadata": {},
   "source": [
    "### Apply phenometrics perpixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfddac-264c-49e6-ac62-458cbf8ac222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lazily loop through spatial indexes and append\n",
    "# returned xarrays to list\n",
    "results=[]\n",
    "for i in range(Y.shape[1]):\n",
    "\n",
    "    #select pixel\n",
    "    data = Y.isel(spatial=i)\n",
    "    \n",
    "    # First, check if spatial index has data. If its one of \n",
    "    # the all-NaN indexes then return xarray filled with -99 values\n",
    "    if i in idx_all_nan:\n",
    "        xx = ds_template.copy() #use our template    \n",
    "        xx['latitude'] = [data.latitude.values.item()] #update coords\n",
    "        xx['longitude'] = [data.longitude.values.item()]\n",
    "    \n",
    "    else:\n",
    "        xx = phenometrics(data,\n",
    "                          rolling=90,\n",
    "                          distance=90,\n",
    "                          prominence='auto',\n",
    "                          plateau_size=10,\n",
    "                          amplitude=0.20\n",
    "                         )\n",
    "\n",
    "    #append results, either data or all-zeros\n",
    "    results.append(xx)\n",
    "\n",
    "#bring into memory\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    results = dask.compute(results)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063fe9b-3cd6-4c77-a6ca-5954fd85eb58",
   "metadata": {},
   "source": [
    "### Find average phenology\n",
    "\n",
    "To do:\n",
    "\n",
    "Add another mtric for the number of seasons\n",
    "\n",
    "df['n_seasons'] = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdff9cc-1058-4f12-a2d1-0d273b507094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "@dask.delayed\n",
    "def _mean(ds):\n",
    "    return ds.mean('index')\n",
    "\n",
    "p_average = [_mean(d) for d in results]\n",
    "p_average = dask.compute(p_average)[0]\n",
    "p_average = xr.combine_by_coords(p_average)\n",
    "\n",
    "#remove NaN areas that have a fill value\n",
    "p_average = p_average.where(p_average>-99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b09aa-51b3-4aa1-9eec-d037449869a2",
   "metadata": {},
   "source": [
    "### Plot average phenology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50aefe1-3e5f-47f8-a191-db4db5ffcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(4,3, figsize=(11,11), sharey=True, sharex=True, layout='constrained')\n",
    "pheno_stats=['SOS','vSOS', 'LOS', \n",
    "             'POS', 'vPOS', 'ROG',\n",
    "             'EOS', 'vEOS', 'ROS',\n",
    "             'IOS', 'vTOS', 'AOS',\n",
    "      ]\n",
    "cmaps = ['twilight', 'gist_earth_r', 'viridis',\n",
    "         'twilight', 'gist_earth_r','magma',\n",
    "         'twilight', 'gist_earth_r', 'magma_r',\n",
    "         'inferno', 'gist_earth_r','plasma'\n",
    "        ]\n",
    "for ax,pheno,cmap in zip(axes.ravel(), pheno_stats, cmaps):\n",
    "    if \"v\" not in pheno:\n",
    "        vmin, vmax=0, 365\n",
    "        label='DOY'\n",
    "    if \"v\" in pheno:\n",
    "        vmin,vmax=0.1, 0.85\n",
    "        label='NDVI'\n",
    "    if 'LOS' in pheno:\n",
    "        vmin, vmax=160, 300\n",
    "        label='days'\n",
    "    if 'AOS' in pheno:\n",
    "        vmin, vmax=0.05, 0.4\n",
    "        label='NDVI'\n",
    "    if 'IOS' in pheno:\n",
    "        vmin, vmax=20, 200\n",
    "        label='NDVI/\\n season'\n",
    "    if 'ROG' in pheno:\n",
    "        vmin, vmax=0.00025, 0.0025\n",
    "        label='NDVI/\\nday'\n",
    "    if 'ROS' in pheno:\n",
    "        vmin, vmax=-0.0025, -0.00025\n",
    "        label='NDVI/\\nday'\n",
    "    im=p_average[pheno].plot(ax=ax, add_colorbar=False, cmap=cmap, vmin=vmin, vmax=vmax, add_labels=False)\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    #need to create colorbar manually to have label on top\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    ax_cbar = fig.colorbar(cbar, ax=ax, shrink=0.7)\n",
    "    ax_cbar.ax.set_title(label, fontsize=8)\n",
    "    ax.set_title(f'{pheno} 1982-2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b393ab-4bbb-415e-a7ab-38fe2383c178",
   "metadata": {},
   "source": [
    "## Find trends in phenology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e0f19-a576-441a-b510-7d37f8e8caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def phenology_trends(ds, vars):\n",
    "    slopes=[]\n",
    "    p_values=[]\n",
    "    for var in vars:\n",
    "\n",
    "        #apply mankendall over 'index' dimension\n",
    "        #this return 9 variables \n",
    "        out = xr.apply_ufunc(mk.original_test,\n",
    "                      ds[var],\n",
    "                      input_core_dims=[[\"index\"]],\n",
    "                      output_core_dims=[[],[],[],[],[],[],[],[],[]],\n",
    "                      vectorize=True)\n",
    "        \n",
    "        #grab just the slope and p-value\n",
    "        p = out[2].rename(var+'_p_value')\n",
    "        s = out[7].rename(var+'_slope')\n",
    "        \n",
    "        slopes.append(s)\n",
    "        p_values.append(p)\n",
    "\n",
    "    #merge all the variables\n",
    "    slopes_xr = xr.merge(slopes)\n",
    "    p_values_xr = xr.merge(p_values)\n",
    "\n",
    "    #export a dataset\n",
    "    return xr.merge([slopes_xr,p_values_xr]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd3415-40a3-40a4-9de4-b1309d8764f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trend_vars = ['POS','vPOS','TOS','vTOS','AOS','SOS',\n",
    "              'vSOS','EOS','vEOS','LOS','IOS','ROG','ROS']\n",
    "p_trends = [phenology_trends(d, trend_vars) for d in results]\n",
    "p_trends = dask.compute(p_trends)[0]\n",
    "p_trends = xr.combine_by_coords(p_trends)\n",
    "\n",
    "#remove NaNs\n",
    "p_trends = p_trends.where(~np.isnan(p_average.vPOS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02b0eb-98b7-4e42-ba4e-87ec86659b3b",
   "metadata": {},
   "source": [
    "### Plot phenology trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a48d54-7154-4ecb-843c-a4114e05c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(4,3, figsize=(11,11),  layout='constrained')#sharey=True, sharex=True,\n",
    "pheno_stats=['SOS','vSOS', 'LOS', \n",
    "             'POS', 'vPOS', 'ROG',\n",
    "             'EOS', 'vEOS', 'ROS',\n",
    "             'AOS', 'vTOS', 'IOS'\n",
    "      ]\n",
    "cmaps = ['coolwarm','BrBG','PRGn',\n",
    "         'coolwarm','BrBG','Spectral',\n",
    "         'coolwarm','BrBG','Spectral_r',\n",
    "         'PiYG','BrBG','PuOr'\n",
    "        ]\n",
    "for ax,pheno,cmap in zip(axes.ravel(), pheno_stats, cmaps):\n",
    "   \n",
    "    if \"v\" not in pheno:\n",
    "        vmin, vmax=-1.5,1.5\n",
    "        label='days/\\nyear'\n",
    "    if \"v\" in pheno:\n",
    "        vmin,vmax=-0.0015, 0.0015\n",
    "        label='NDVI/\\nyear'\n",
    "    if 'LOS' in pheno:\n",
    "        vmin, vmax=-1.5, 1.5\n",
    "        label='days/\\nyear'\n",
    "    if 'AOS' in pheno:\n",
    "        vmin, vmax=-0.002, 0.002\n",
    "        label='NDVI\\nyear'\n",
    "    if 'ROG' in pheno:\n",
    "        vmin, vmax=-2.0e-05, 2.0e-05\n",
    "        label='NDVI/day/\\nyear'\n",
    "    if 'ROS' in pheno:\n",
    "        vmin, vmax=-2.0e-05, 2.0e-05\n",
    "        label='NDVI/day/\\nyear'\n",
    "    if 'IOS' in pheno:\n",
    "        vmin, vmax=-0.5, 0.5\n",
    "        label='NDVI/\\nyear'\n",
    "\n",
    "    d_to_plot = p_trends[pheno+'_slope']    \n",
    "    im=d_to_plot.plot(ax=ax, add_colorbar=False, cmap=cmap, vmin=vmin, vmax=vmax, add_labels=False)\n",
    "                             \n",
    "    # significance plotting\n",
    "    lons, lats = np.meshgrid(d_to_plot.longitude, d_to_plot.latitude)\n",
    "    sig_area = np.where(p_trends[pheno+'_p_value'] <= 0.05)\n",
    "    ax.hexbin(x=lons[sig_area].reshape(-1),\n",
    "             y=lats[sig_area].reshape(-1),\n",
    "             C=d_to_plot.where(p_trends[pheno+'_p_value'] <= 0.05).data[sig_area].reshape(-1),\n",
    "             hatch='XXXX',\n",
    "             alpha=0,\n",
    "             gridsize=50#(10,10)\n",
    "            )    \n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.VoyagerNoLabels, crs='EPSG:4326', attribution='', attribution_size=1)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "    #need to create colorbar manually to have label on top\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    ax_cbar = fig.colorbar(cbar, ax=ax, shrink=0.7)\n",
    "    ax_cbar.ax.set_title(label, fontsize=8)\n",
    "    ax.set_title(f'{pheno}, 1982-2022')\n",
    "    # print(pheno)\n",
    "    \n",
    "# axes[3,2].axis('off');\n",
    "# fig.savefig(f'/g/data/os22/chad_tmp/Aus_phenology/results/figs/trends_phenometrics_map_{region_type}_{product}.png',\n",
    "#             bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce5d53-3fef-4b1c-ab6a-8b759f33ea37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f320f3-e42a-4ee4-a782-bb289943c9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1050ea-f5a2-4aa2-a00b-280c6e997929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ce19b-8699-40dc-9976-e68b288e4a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855bcb4-785a-4d2c-a505-d5823123b01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8842e19-2779-45ce-999e-796e79db9a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d61ca-ee32-49b2-8ac1-497b48b4c804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b67ebe-4554-49b2-a52e-4f6c7a10c7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b2eb6-6f86-416e-ad35-e2420ad5c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "416a0745-623c-45f5-b7f9-ea7fb85f6f7a",
   "metadata": {},
   "source": [
    "## Regression between annual means and climate\n",
    "\n",
    "testing per pixel PLS regression with dask delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51185f2c-1baa-4d82-9618-ce7dcfd731e8",
   "metadata": {},
   "source": [
    "### Annual means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187b372-0570-4f30-8746-28a5fe090909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.resample(time='YS').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab55ce9e-337a-4d29-a233-112143d86af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rain = covars['rain']\n",
    "# rain = rain.resample(time='YS').sum()\n",
    "\n",
    "# covars = covars.drop('rain')\n",
    "# covars = covars.resample(time='YS').mean()\n",
    "# covars['rain'] = rain\n",
    "\n",
    "# covars = covars.drop('trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea62f8-6b36-4123-b90a-3a2cb2c7dd00",
   "metadata": {},
   "source": [
    "### test dask.delayed method\n",
    "\n",
    "This works fast but it has a serious memory leak...RAM explodes once you get close to 10,0000 pixels. ~100 GiB to run a 100x100 tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf59eca-793a-4269-850c-cc0f3fa2c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f499a-a1d7-48c1-83ab-93586bf25994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_pls_regression_dask(x, y, n_components=2):\n",
    "#     \"\"\"\n",
    "#     Perform PLS regression along the time dimension of two xarray DataArrays using Dask for parallelization.\n",
    "    \n",
    "#     Parameters:\n",
    "#     x (xr.DataArray): The independent variables with dimensions (time, latitude, longitude, variable).\n",
    "#     y (xr.DataArray): The dependent variable with dimensions (time, latitude, longitude).\n",
    "#     n_components (int): Number of PLS components to use.\n",
    "    \n",
    "#     Returns:\n",
    "#     xr.Dataset: A dataset containing the PLS regression coefficients, and R-squared values.\n",
    "#     \"\"\"\n",
    "#     # Ensure x has the shape (time, latitude, longitude, variables)\n",
    "#     if 'variable' not in x.dims:\n",
    "#         raise ValueError(\"x must have a 'variable' dimension representing independent variables.\")\n",
    "    \n",
    "#     # Align the arrays to ensure they have matching time, latitude, longitude dimensions\n",
    "#     x, y = xr.align(x, y)\n",
    "    \n",
    "#     # Stack latitude and longitude into a single dimension to simplify the regression calculation\n",
    "#     x_stack = x.stack(spatial=('latitude', 'longitude'))\n",
    "#     y_stack = y.stack(spatial=('latitude', 'longitude'))\n",
    "\n",
    "#     # Convert to numpy arrays or dask arrays\n",
    "#     X = x_stack.transpose('time', 'spatial', 'variable').data\n",
    "#     Y = y_stack.transpose('time', 'spatial').data\n",
    "\n",
    "#     # Perform the PLS regression at each spatial point using Dask\n",
    "#     def compute_pls(i):\n",
    "#         Xi = X[:, i, :]\n",
    "#         Yi = Y[:, i]\n",
    "\n",
    "#         #check for NaNs\n",
    "#         mask = ~np.isnan(Yi)\n",
    "        \n",
    "#         Xi = Xi[mask]\n",
    "#         Yi = Yi[mask]\n",
    "        \n",
    "#         # Initialize the PLS regression model\n",
    "#         pls = PLSRegression(n_components=n_components)\n",
    "        \n",
    "#         # Fit the model\n",
    "#         pls.fit(Xi, Yi)\n",
    "\n",
    "#          #predict\n",
    "#         Y_fit = pls.predict(Xi)\n",
    "\n",
    "#         #find the slope of predicted and original\n",
    "#         s_pred = mk.original_test(Y_fit, alpha=0.05)\n",
    "#         s_actual = mk.original_test(Yi, alpha=0.05)\n",
    "\n",
    "#         r2 = r2_score(Yi, Y_fit)\n",
    "        \n",
    "#         return pls.coef_.ravel(), r2, s_pred.slope, s_actual.slope\n",
    "\n",
    "#     # Use Dask to parallelize the computation across spatial points\n",
    "#     results = dask.array.compute([delayed(compute_pls)(i) for i in range(X.shape[1])])\n",
    "\n",
    "#     # Unpack results\n",
    "#     coefs, r_squared, slopes_pred, slopes_actual = zip(*results[0])\n",
    "    \n",
    "#     # Convert lists to arrays\n",
    "#     coefs = np.array(coefs).T\n",
    "#     r_squared = np.array(r_squared)\n",
    "#     slopes_pred = np.array(slopes_pred)\n",
    "#     slopes_actual = np.array(slopes_actual)\n",
    "    \n",
    "#     #retrun the results as xarrays\n",
    "#     coefs = xr.DataArray(coefs, dims=('variable', 'spatial'),\n",
    "#                          coords={'variable': x_stack.mean('time').variable,\n",
    "#                                   'spatial': x_stack.spatial})\n",
    "#     coefs = coefs.unstack('spatial')\n",
    "#     coefs =  xr.Dataset({\n",
    "#                 'CO2':coefs.variable[0],\n",
    "#                 'srad':coefs.variable[1],\n",
    "#                 'tavg':coefs.variable[2],\n",
    "#                 'vpd':coefs.variable[3],\n",
    "#                 'rain':coefs.variable[4]},\n",
    "#                 coords={'latitude': coefs.latitude,\n",
    "#                               'longitude':coefs.longitude})\n",
    "    \n",
    "#     r_squared = xr.DataArray(r_squared, dims=('spatial',), coords={'spatial': x_stack.spatial})\n",
    "#     r_squared = r_squared.unstack('spatial')\n",
    "    \n",
    "#     slopes_pred = xr.DataArray(slopes_pred, dims=('spatial',), coords={'spatial': x_stack.spatial})\n",
    "#     slopes_pred = slopes_pred.unstack('spatial')\n",
    "    \n",
    "#     slopes_actual = xr.DataArray(slopes_actual, dims=('spatial',), coords={'spatial': x_stack.spatial})\n",
    "#     slopes_actual = slopes_actual.unstack('spatial')\n",
    "    \n",
    "#     r = xr.Dataset({\n",
    "#         'r_squared': r_squared,\n",
    "#         'slopes_predicted': slopes_pred,\n",
    "#         'slopes_actual': slopes_actual,\n",
    "#     })\n",
    "    \n",
    "#     return coefs, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4753af-3a9e-43f4-af2d-43a2e392afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = ds.isel(latitude=slice(200,300), longitude=slice(200,300))\n",
    "# covars_test = covars.isel(latitude=slice(200,300), longitude=slice(200,300)).to_dataarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c5f3e-b02f-47ef-9ad5-6ae0a9124035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# c, r = apply_pls_regression_dask(covars_test, ds, n_components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08d5df-4661-4ae8-b7b5-7df3bf3653ce",
   "metadata": {},
   "source": [
    "## test ufunc\n",
    "\n",
    "doesn't seem to run??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb824d-8916-46e1-961b-eb15aeb2489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = ds.isel(latitude=slice(200,300), longitude=slice(200,300))\n",
    "# covars_test = covars.isel(latitude=slice(200,300), longitude=slice(200,300)).to_dataarray()\n",
    "\n",
    "# ds_test = ds_test.chunk({'latitude': 20, 'longitude': 20})\n",
    "# covars_test = covars_test.chunk({'latitude': 20, 'longitude': 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5faa-c743-4d73-ac4f-d180d0678430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Example usage (assuming your data is already loaded into x and y):\n",
    "# result = apply_pls_regression_xarray(covars_test, ds_test, n_components=2)#.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31589c51-6c43-40de-bce8-4d8ccd96aa75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
